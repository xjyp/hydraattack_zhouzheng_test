#!/usr/bin/env python3
"""
DQNè·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°è„šæœ¬
åŠ è½½åœ¨ä¸€ä¸ªbenchmarkä¸Šè®­ç»ƒçš„DQNæ¨¡å‹ï¼Œåœ¨å¦å¤–ä¸¤ä¸ªbenchmarkçš„æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•
"""

import argparse
import sys
import json
import time
import os
import numpy as np
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

from data_types import PairwiseExample
from attacks import (
    FlipAttackFWO, FlipAttackFCW, FlipAttackFCS, UncertaintyAttack, PositionAttack, DistractorAttack,
    PromptInjectionAttack, MarkerInjectionAttack, FormattingAttack,
    AuthorityAttack, UnicodeAttack, CoTPoisoningAttack, EmojiAttack
)
from evaluation.qwen_judge import create_qwen_judge
from evaluation.llama_judge import create_llama_judge
from rl.enhanced_environment import EnhancedHydraAttackEnv
from rl.agent import DQNAgent
from utils.logger import HydraLogger


def convert_numpy_types(obj):
    """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç”¨äºJSONåºåˆ—åŒ–"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj


def load_benchmark_data(benchmark: str, max_samples = 100, random_seed: int = 42) -> List[PairwiseExample]:
    """åŠ è½½benchmarkæµ‹è¯•æ•°æ®"""
    test_file = f"data/split/{benchmark}_test.json"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_file}")
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_file}")
    
    with open(test_file, 'r') as f:
        data = json.load(f)
    
    print(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®: {len(data)} æ ·æœ¬")
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if max_samples != "full" and max_samples is not None:
        try:
            max_samples_int = int(max_samples)
            if len(data) > max_samples_int:
                data = data[:max_samples_int]
                print(f"âœ… é™åˆ¶æ ·æœ¬æ•°é‡ä¸º: {max_samples_int}")
        except (ValueError, TypeError):
            # å¦‚æœmax_samplesæ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¿½ç•¥é™åˆ¶
            pass
    
    examples = []
    for sample in data:
        example = PairwiseExample(
            question_id=sample["question_id"],
            instruction=sample["instruction"],
            response_a=sample["response_a"],
            response_b=sample["response_b"],
            model_a=sample["model_a"],
            model_b=sample["model_b"],
            metadata=sample.get("metadata", {})
        )
        examples.append(example)
    
    return examples


def get_attack_methods() -> List[Any]:
    """è·å–æ‰€æœ‰æ”»å‡»æ–¹æ³•å®ä¾‹"""
    attacks = [
        FlipAttackFCS(),
        FlipAttackFWO(),
        FlipAttackFCW(),
        UncertaintyAttack(),
        PositionAttack(),
        DistractorAttack(),
        PromptInjectionAttack(),
        MarkerInjectionAttack(),
        FormattingAttack(),
        AuthorityAttack(),
        UnicodeAttack(),
        CoTPoisoningAttack(),
        EmojiAttack(),
    ]
    return attacks


def load_dqn_agent(model_path: str, config_path: str, logger: HydraLogger = None) -> DQNAgent:
    """åŠ è½½è®­ç»ƒå¥½çš„DQNæ™ºèƒ½ä½“"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # è·å–DQNå‚æ•°
    dqn_params = config.get('dqn_params', {})
    env_params = config.get('env_params', {})
    
    # åˆ›å»ºæ”»å‡»æ–¹æ³•ä»¥è·å–åŠ¨ä½œç©ºé—´å¤§å°
    attacks = get_attack_methods()
    total_actions = sum(attack.get_action_space_size() for attack in attacks)
    
    # ä»ä¿å­˜çš„æ¨¡å‹ä¸­æ¨æ–­çŠ¶æ€ç»´åº¦
    import torch
    checkpoint = torch.load(model_path, map_location='cpu')
    
    # ä»Qç½‘ç»œçš„ç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„æƒé‡å½¢çŠ¶æ¨æ–­çŠ¶æ€ç»´åº¦
    if 'q_network_state_dict' in checkpoint:
        state_dict = checkpoint['q_network_state_dict']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„æƒé‡ï¼ˆé€šå¸¸æ˜¯'0.weight'ï¼‰
    first_layer_weight = None
    for key, value in state_dict.items():
        if 'weight' in key and len(value.shape) == 2 and key.startswith('0.'):
            first_layer_weight = value
            break
    
    if first_layer_weight is not None:
        state_dim = first_layer_weight.shape[1]  # è¾“å…¥ç»´åº¦
    else:
        state_dim = 512  # é»˜è®¤å€¼
    
    if logger:
        logger.logger.info(f"   æ¨æ–­çš„çŠ¶æ€ç»´åº¦: {state_dim}")
    
    # åˆ›å»ºDQNæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨æ¨æ–­çš„çŠ¶æ€ç»´åº¦ï¼‰
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=total_actions,
        hidden_dim=dqn_params.get('hidden_dim', 512),
        learning_rate=dqn_params.get('learning_rate', 0.0001),
        gamma=dqn_params.get('gamma', 0.99),
        epsilon=0.0,  # è¯„ä¼°æ—¶ä½¿ç”¨è´ªå©ªç­–ç•¥
        epsilon_min=0.0,
        epsilon_decay=1.0,
        batch_size=dqn_params.get('batch_size', 32),
        memory_size=dqn_params.get('memory_size', 50000),
        target_update_freq=dqn_params.get('target_update_freq', 500)
    )
    
    # åŠ è½½æ¨¡å‹æƒé‡
    agent.load_model(model_path)
    
    if logger:
        logger.logger.info(f"âœ… æˆåŠŸåŠ è½½DQNæ¨¡å‹: {model_path}")
        logger.logger.info(f"   çŠ¶æ€ç»´åº¦: {state_dim}")
        logger.logger.info(f"   åŠ¨ä½œç©ºé—´å¤§å°: {total_actions}")
        logger.logger.info(f"   éšè—å±‚ç»´åº¦: {dqn_params.get('hidden_dim', 512)}")
        logger.logger.info(f"   å­¦ä¹ ç‡: {dqn_params.get('learning_rate', 0.0001)}")
    
    return agent


def evaluate_cross_benchmark_performance(agent, test_examples: List[PairwiseExample], 
                                       attacks: List[Any], judge, max_queries: int = 10,
                                       success_reward: float = 20.0, query_penalty: float = 0.5,
                                       diversity_bonus: float = 1.0, efficiency_bonus: float = 2.0,
                                       confidence_threshold: float = 0.7,
                                       logger: HydraLogger = None) -> Dict[str, float]:
    """è¯„ä¼°è·¨æ•°æ®é›†æ³›åŒ–æ€§èƒ½"""
    
    if logger:
        logger.logger.info("ğŸ” å¼€å§‹è·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°...")
        logger.logger.info(f"   ç›®æ ‡Judge: {judge.__class__.__name__}")
        logger.logger.info(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(test_examples)}")
    
    success_count = 0
    total_queries = 0
    total_reward = 0.0
    detailed_results = []
    
    for i, example in enumerate(test_examples):
        if logger and i % 10 == 0:
            logger.logger.info(f"  è¯„ä¼°è¿›åº¦: {i+1}/{len(test_examples)}")
        
        # 1. å…ˆè·å–åŸå§‹åå¥½ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        try:
            original_response = judge.judge_pairwise(example)
            original_preference = original_response.preference
            original_confidence = original_response.confidence
        except Exception as e:
            if logger:
                logger.logger.warning(f"è·å–åŸå§‹åå¥½å¤±è´¥: {e}")
            continue  # è·³è¿‡å¤±è´¥çš„æ ·æœ¬
        
        # 2. åˆ›å»ºç¯å¢ƒ
        env = EnhancedHydraAttackEnv(
            examples=[example],
            attacks=attacks,
            judge=judge,
            max_queries=max_queries,
            success_reward=success_reward,
            query_penalty=query_penalty,
            diversity_bonus=diversity_bonus,
            efficiency_bonus=efficiency_bonus,
            confidence_threshold=confidence_threshold
        )
        
        # 3. é‡ç½®ç¯å¢ƒï¼ˆè¯„ä¼°æ¨¡å¼ï¼šä½¿ç”¨ç‰¹å®šæ ·æœ¬å’Œé¢„å…ˆè·å–çš„åå¥½ï¼‰
        state, _ = env.reset(options={
            'use_specific_sample': True,
            'sample_idx': 0,
            'original_preference': original_preference,
            'original_confidence': original_confidence
        })
        episode_reward = 0
        queries_used = 0
        attack_sequence = []
        
        # 4. æ‰§è¡Œæ”»å‡»åºåˆ—
        for step in range(max_queries):
            # è·å–action maskï¼ˆç¦æ­¢é‡å¤ä½¿ç”¨å¤±è´¥çš„actionï¼‰
            action_mask = env.get_action_mask()
            
            # ä½¿ç”¨è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œï¼ˆè´ªå©ªç­–ç•¥ï¼‰
            action = agent.select_action(state, training=False, action_mask=action_mask)
            next_state, reward, done, truncated, info = env.step(action)
            
            # è®°å½•æ”»å‡»ä¿¡æ¯
            attack_info = {
                'step': step + 1,
                'action': int(action),
                'reward': float(reward),
                'done': done,
                'truncated': truncated
            }
            attack_sequence.append(attack_info)
            
            episode_reward += reward
            queries_used += 1
            
            if done or truncated:
                break
            
            state = next_state
        
        # 5. æ£€æŸ¥æ”»å‡»ç»“æœ
        original_preference = env.original_preference
        final_preference = info.get('current_preference', original_preference)
        # ä½¿ç”¨ç¯å¢ƒè¿”å›çš„successä¿¡æ¯ï¼Œå®ƒå·²ç»è€ƒè™‘äº†PositionAttackçš„ç‰¹æ®Šé€»è¾‘
        success = info.get('success', False)
        
        if success:
            success_count += 1
        
        total_queries += queries_used
        total_reward += episode_reward
        
        # è®°å½•è¯¦ç»†ç»“æœ
        sample_result = {
            'question_id': example.question_id,
            'success': success,
            'queries_used': queries_used,
            'episode_reward': episode_reward,
            'original_preference': original_preference,
            'final_preference': final_preference,
            'original_confidence': original_confidence,
            'final_confidence': info.get('current_confidence', original_confidence),
            'attack_sequence': attack_sequence
        }
        detailed_results.append(sample_result)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    success_rate = success_count / len(test_examples) if test_examples else 0.0
    avg_queries = total_queries / len(test_examples) if test_examples else 0.0
    avg_reward = total_reward / len(test_examples) if test_examples else 0.0
    
    results = {
        'success_rate': success_rate,
        'avg_queries': avg_queries,
        'avg_reward': avg_reward,
        'total_samples': len(test_examples),
        'successful_attacks': success_count,
        'detailed_results': detailed_results
    }
    
    if logger:
        logger.logger.info(f"ğŸ“Š è·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°ç»“æœ:")
        logger.logger.info(f"  æˆåŠŸç‡: {success_rate:.3f}")
        logger.logger.info(f"  å¹³å‡æŸ¥è¯¢æ¬¡æ•°: {avg_queries:.3f}")
        logger.logger.info(f"  å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
        logger.logger.info(f"  æˆåŠŸæ”»å‡»æ•°: {success_count}/{len(test_examples)}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DQNè·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°")
    parser.add_argument("--source_benchmark", type=str, required=True,
                       choices=["alpaca_eval", "arena_hard", "code_judge_bench"],
                       help="è®­ç»ƒDQNæ¨¡å‹çš„æºbenchmark")
    parser.add_argument("--target_benchmark", type=str, required=True,
                       choices=["alpaca_eval", "arena_hard", "code_judge_bench"],
                       help="æµ‹è¯•çš„ç›®æ ‡benchmark")
    parser.add_argument("--max_samples", default=100, help="æµ‹è¯•æ ·æœ¬æ•°ï¼Œä½¿ç”¨ 'full' è¡¨ç¤ºä½¿ç”¨å…¨é‡æ•°æ®")
    parser.add_argument("--judge_model_path", type=str, required=True, help="Judgeæ¨¡å‹è·¯å¾„")
    parser.add_argument("--judge_type", type=str, default="qwen", help="Judgeç±»å‹")
    parser.add_argument("--dqn_model_path", type=str, required=True, help="DQNæ¨¡å‹è·¯å¾„")
    parser.add_argument("--dqn_config_path", type=str, required=True, help="DQNé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max_queries", type=int, default=10, help="æœ€å¤§æŸ¥è¯¢æ¬¡æ•°")
    parser.add_argument("--output_dir", type=str, default="./cross_benchmark_results", help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--random_seed", type=int, default=42, help="éšæœºç§å­")
    
    # ç¯å¢ƒå¥–åŠ±å‚æ•°
    parser.add_argument("--success_reward", type=float, default=20.0, help="æˆåŠŸæ”»å‡»å¥–åŠ±")
    parser.add_argument("--query_penalty", type=float, default=0.5, help="æŸ¥è¯¢æƒ©ç½š")
    parser.add_argument("--diversity_bonus", type=float, default=1.0, help="å¤šæ ·æ€§å¥–åŠ±")
    parser.add_argument("--efficiency_bonus", type=float, default=2.0, help="æ•ˆç‡å¥–åŠ±")
    parser.add_argument("--confidence_threshold", type=float, default=0.7, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æºbenchmarkå’Œç›®æ ‡benchmarkä¸èƒ½ç›¸åŒ
    if args.source_benchmark == args.target_benchmark:
        print("âŒ æºbenchmarkå’Œç›®æ ‡benchmarkä¸èƒ½ç›¸åŒ")
        return
    
    # è®¾ç½®éšæœºç§å­
    import random
    random.seed(args.random_seed)
    np.random.seed(args.random_seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    logger = HydraLogger(log_dir=args.output_dir, results_dir=args.output_dir)
    
    logger.logger.info("ğŸš€ å¼€å§‹DQNè·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°")
    logger.logger.info(f"æºbenchmark: {args.source_benchmark}")
    logger.logger.info(f"ç›®æ ‡benchmark: {args.target_benchmark}")
    logger.logger.info(f"Judgeæ¨¡å‹: {args.judge_model_path}")
    logger.logger.info(f"Judgeç±»å‹: {args.judge_type}")
    logger.logger.info(f"DQNæ¨¡å‹: {args.dqn_model_path}")
    logger.logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {args.max_samples}")
    logger.logger.info(f"æœ€å¤§æŸ¥è¯¢æ¬¡æ•°: {args.max_queries}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_examples = load_benchmark_data(args.target_benchmark, args.max_samples, args.random_seed)
    if not test_examples:
        logger.logger.error("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
        return
    
    logger.logger.info(f"âœ… åŠ è½½äº† {len(test_examples)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # åˆ›å»ºæ”»å‡»æ–¹æ³•
    attacks = get_attack_methods()
    logger.logger.info(f"âœ… åŠ è½½äº† {len(attacks)} ç§æ”»å‡»æ–¹æ³•")
    
    # åˆ›å»ºJudgeï¼ˆç›®æ ‡æ¨¡å‹ï¼‰
    try:
        if args.judge_type == "qwen":
            judge = create_qwen_judge(args.judge_model_path)
        elif args.judge_type == "llama":
            judge = create_llama_judge(args.judge_model_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„Judgeç±»å‹: {args.judge_type}")
        logger.logger.info("âœ… Judgeåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.logger.error(f"âŒ Judgeåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åŠ è½½DQNæ™ºèƒ½ä½“
    try:
        agent = load_dqn_agent(args.dqn_model_path, args.dqn_config_path, logger)
        logger.logger.info("âœ… DQNæ™ºèƒ½ä½“åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.logger.error(f"âŒ DQNæ™ºèƒ½ä½“åŠ è½½å¤±è´¥: {e}")
        return
    
    # å¼€å§‹è¯„ä¼°
    logger.logger.info("ğŸ¯ å¼€å§‹è·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°...")
    start_time = time.time()
    
    results = evaluate_cross_benchmark_performance(
        agent, test_examples, attacks, judge, args.max_queries,
        args.success_reward, args.query_penalty, args.diversity_bonus,
        args.efficiency_bonus, args.confidence_threshold, logger
    )
    
    evaluation_time = time.time() - start_time
    logger.logger.info(f"ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    logger.logger.info(f"â±ï¸  è¯„ä¼°è€—æ—¶: {evaluation_time:.2f}ç§’")
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results_file = os.path.join(args.output_dir, f"cross_benchmark_evaluation_{args.source_benchmark}_to_{args.target_benchmark}_{timestamp}.json")
    
    # å‡†å¤‡ä¿å­˜çš„ç»“æœï¼ˆä¸åŒ…å«è¯¦ç»†ç»“æœä»¥èŠ‚çœç©ºé—´ï¼‰
    save_results = {
        'source_benchmark': args.source_benchmark,
        'target_benchmark': args.target_benchmark,
        'judge_model_path': args.judge_model_path,
        'judge_type': args.judge_type,
        'dqn_model_path': args.dqn_model_path,
        'dqn_config_path': args.dqn_config_path,
        'max_samples': args.max_samples,
        'max_queries': args.max_queries,
        'evaluation_time': evaluation_time,
        'success_rate': results['success_rate'],
        'avg_queries': results['avg_queries'],
        'avg_reward': results['avg_reward'],
        'total_samples': results['total_samples'],
        'successful_attacks': results['successful_attacks'],
        'env_params': {
            'success_reward': args.success_reward,
            'query_penalty': args.query_penalty,
            'diversity_bonus': args.diversity_bonus,
            'efficiency_bonus': args.efficiency_bonus,
            'confidence_threshold': args.confidence_threshold
        }
    }
    
    # è½¬æ¢numpyç±»å‹
    save_results = convert_numpy_types(save_results)
    
    with open(results_file, 'w') as f:
        json.dump(save_results, f, indent=2)
    
    logger.logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    logger.logger.info("=" * 80)
    logger.logger.info("ğŸ“Š è·¨æ•°æ®é›†æ³›åŒ–è¯„ä¼°ç»“æœ")
    logger.logger.info("=" * 80)
    logger.logger.info(f"æºbenchmark: {args.source_benchmark}")
    logger.logger.info(f"ç›®æ ‡benchmark: {args.target_benchmark}")
    logger.logger.info(f"Judgeæ¨¡å‹: {args.judge_type}")
    logger.logger.info(f"æˆåŠŸç‡: {results['success_rate']:.3f}")
    logger.logger.info(f"å¹³å‡æŸ¥è¯¢æ¬¡æ•°: {results['avg_queries']:.3f}")
    logger.logger.info(f"å¹³å‡å¥–åŠ±: {results['avg_reward']:.3f}")
    logger.logger.info(f"æˆåŠŸæ”»å‡»æ•°: {results['successful_attacks']}/{results['total_samples']}")
    logger.logger.info("=" * 80)
    
    return results


if __name__ == "__main__":
    main()
