#!/usr/bin/env python3
"""
æ”¹è¿›çš„å¿«é€ŸRLè®­ç»ƒè„šæœ¬ - Rainbow DQNç‰ˆæœ¬ï¼ˆæ”¯æŒæ•°æ®åˆ†å‰²å’Œæœªè§æ•°æ®è¯„ä¼°ï¼‰
"""

import argparse
import sys
import json
import time
import os
import numpy as np
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

from data_types import PairwiseExample, RLConfig
from attacks import (
    FlipAttackFWO, FlipAttackFCW, FlipAttackFCS, UncertaintyAttack, PositionAttack, DistractorAttack,
    PromptInjectionAttack, MarkerInjectionAttack, FormattingAttack,
    AuthorityAttack, UnicodeAttack, CoTPoisoningAttack, EmojiAttack
)
from evaluation.qwen_judge import create_qwen_judge
from evaluation.llama_judge import create_llama_judge
from evaluation.glm_judge import create_glm_judge
from evaluation.mistral_judge import create_mistral_judge
from evaluation.gemma_judge import create_gemma_judge
from evaluation.gemma_judge1b import create_gemma_judge as create_gemma_judge_1b
from rl.enhanced_environment import EnhancedHydraAttackEnv
from rl.agent import RainbowDQNAgent
from rl.trainer import RLTrainer, evaluate_on_unseen_data
from utils.logger import HydraLogger


def convert_numpy_types(obj):
    """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç”¨äºJSONåºåˆ—åŒ–"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj


def load_benchmark_data(benchmark: str, max_samples = 100, random_seed: int = 42, validation_ratio: float = 0.1) -> tuple[List[PairwiseExample], List[PairwiseExample]]:
    """åŠ è½½benchmarkæ•°æ®å¹¶åˆ†å‡ºéªŒè¯é›†ï¼ˆå¿…é¡»ä½¿ç”¨é¢„åˆ†å‰²æ•°æ®ï¼‰"""
    # å¿…é¡»ä½¿ç”¨é¢„åˆ†å‰²çš„è®­ç»ƒæ•°æ®
    train_file = f"data/split/{benchmark}_train.json"
    
    if not os.path.exists(train_file):
        print(f"âŒ é¢„åˆ†å‰²çš„è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_file}")
        print("è¯·å…ˆè¿è¡Œ prepare_data.py è„šæœ¬å‡†å¤‡æ•°æ®åˆ†å‰²")
        raise FileNotFoundError(f"é¢„åˆ†å‰²çš„è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_file}")
    
    # ä½¿ç”¨é¢„åˆ†å‰²çš„è®­ç»ƒæ•°æ®
    with open(train_file, 'r') as f:
        data = json.load(f)
    
    print(f"âœ… ä½¿ç”¨é¢„åˆ†å‰²çš„è®­ç»ƒæ•°æ®: {len(data)} æ ·æœ¬")
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if max_samples != "full" and max_samples is not None:
        try:
            max_samples_int = int(max_samples)
            if len(data) > max_samples_int:
                data = data[:max_samples_int]
        except (ValueError, TypeError):
            # å¦‚æœmax_samplesæ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¿½ç•¥é™åˆ¶
            pass
    
    # è½¬æ¢ä¸ºPairwiseExampleå¯¹è±¡
    examples = []
    for sample in data:
        example = PairwiseExample(
            question_id=sample["question_id"],
            instruction=sample["instruction"],
            response_a=sample["response_a"],
            response_b=sample["response_b"],
            model_a=sample["model_a"],
            model_b=sample["model_b"],
            metadata=sample.get("metadata", {})
        )
        examples.append(example)
    
    # ä»è®­ç»ƒé›†ä¸­åˆ†å‡ºéªŒè¯é›†
    if validation_ratio > 0 and len(examples) > 1:
        np.random.seed(random_seed)
        n_total = len(examples)
        n_validation = int(n_total * validation_ratio)
        
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(n_total)
        
        # åˆ†å‡ºéªŒè¯é›†å’Œè®­ç»ƒé›†
        validation_indices = indices[:n_validation]
        train_indices = indices[n_validation:]
        
        train_examples = [examples[i] for i in train_indices]
        validation_examples = [examples[i] for i in validation_indices]
        
        print(f"âœ… æ•°æ®åˆ†å‰²: è®­ç»ƒé›† {len(train_examples)} æ ·æœ¬, éªŒè¯é›† {len(validation_examples)} æ ·æœ¬")
        
        return train_examples, validation_examples
    else:
        print(f"âœ… ä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒé›†: {len(examples)} æ ·æœ¬")
        return examples, []




def get_attack_methods() -> List[Any]:
    """è·å–æ‰€æœ‰æ”»å‡»æ–¹æ³•å®ä¾‹"""
    attacks = [
        FlipAttackFCS(),
        FlipAttackFWO(),
        FlipAttackFCW(),
        UncertaintyAttack(),
        PositionAttack(),
        DistractorAttack(),
        PromptInjectionAttack(),
        MarkerInjectionAttack(),
        FormattingAttack(),
        AuthorityAttack(),
        UnicodeAttack(),
        CoTPoisoningAttack(),
        EmojiAttack(),
    ]
    return attacks



def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒå¿«é€ŸRLæ”»å‡»æ™ºèƒ½ä½“ - Rainbow DQNï¼ˆæ”¯æŒæ•°æ®åˆ†å‰²ï¼‰")
    parser.add_argument("--benchmark", type=str, default="alpaca_eval", 
                       choices=["alpaca_eval", "arena_hard", "code_judge_bench", "mtbench", "llmbar"],
                       help="ä½¿ç”¨çš„benchmark")
    parser.add_argument("--max_samples", default=100, help="æ€»æ ·æœ¬æ•°ï¼Œä½¿ç”¨ 'full' è¡¨ç¤ºä½¿ç”¨å…¨é‡æ•°æ®")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--test_ratio", type=float, default=0.2, help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--validation_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»è®­ç»ƒé›†ä¸­åˆ†å‡ºï¼‰")
    parser.add_argument("--judge_model_path", type=str, required=True, help="Judgeæ¨¡å‹è·¯å¾„")
    parser.add_argument("--judge_type", type=str, default="qwen", help="Judgeç±»å‹")
    parser.add_argument("--episodes", type=int, default=1000, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--max_queries", type=int, default=10, help="è®­ç»ƒæ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°")
    parser.add_argument("--max_queries_test", type=int, default=None, help="æµ‹è¯•æ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°ï¼ˆé»˜è®¤ä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰")
    parser.add_argument("--learning_rate", type=float, default=0.001, help="å­¦ä¹ ç‡")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--model_dir", type=str, default="./models", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--log_dir", type=str, default="./logs", help="æ—¥å¿—ç›®å½•")
    parser.add_argument("--random_seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--eval_freq", type=int, default=500, help="éªŒè¯é¢‘ç‡ï¼ˆæ¯å¤šå°‘ä¸ªepisodeè¯„ä¼°ä¸€æ¬¡ï¼‰")
    parser.add_argument("--early_stopping_patience", type=int, default=10, help="Early stopping patience")
    parser.add_argument("--early_stopping_min_delta", type=float, default=0.001, help="Early stoppingæœ€å°æ”¹å–„é˜ˆå€¼")
    
    # Rainbow DQNç®—æ³•ç‰¹æœ‰å‚æ•°
    parser.add_argument("--hidden_dim", type=int, default=512, help="ç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦")
    parser.add_argument("--gamma", type=float, default=0.99, help="æŠ˜æ‰£å› å­")
    parser.add_argument("--epsilon", type=float, default=1.0, help="åˆå§‹æ¢ç´¢ç‡")
    parser.add_argument("--epsilon_min", type=float, default=0.01, help="æœ€å°æ¢ç´¢ç‡")
    parser.add_argument("--epsilon_decay", type=float, default=0.9995, help="æ¢ç´¢ç‡è¡°å‡å› å­")
    parser.add_argument("--memory_size", type=int, default=50000, help="ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°")
    parser.add_argument("--target_update_freq", type=int, default=500, help="ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡")
    
    # Rainbow DQNç‰¹æœ‰å‚æ•°
    parser.add_argument("--prioritized_replay", type=str, default="true", help="æ˜¯å¦ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾ (true/false)")
    parser.add_argument("--prioritized_replay_alpha", type=float, default=0.6, help="ä¼˜å…ˆç»éªŒå›æ”¾alphaå‚æ•°")
    parser.add_argument("--prioritized_replay_beta", type=float, default=0.4, help="ä¼˜å…ˆç»éªŒå›æ”¾betaå‚æ•°")
    parser.add_argument("--prioritized_replay_beta_increment", type=float, default=0.001, help="ä¼˜å…ˆç»éªŒå›æ”¾betaå¢é‡")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰")
    
    # ç¯å¢ƒå¥–åŠ±å‚æ•°
    parser.add_argument("--success_reward", type=float, default=20.0, help="æˆåŠŸæ”»å‡»å¥–åŠ±")
    parser.add_argument("--query_penalty", type=float, default=0.5, help="æŸ¥è¯¢æƒ©ç½š")
    parser.add_argument("--diversity_bonus", type=float, default=1.0, help="å¤šæ ·æ€§å¥–åŠ±")
    parser.add_argument("--efficiency_bonus", type=float, default=2.0, help="æ•ˆç‡å¥–åŠ±")
    parser.add_argument("--confidence_threshold", type=float, default=0.7, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    import random
    import numpy as np
    random.seed(args.random_seed)
    np.random.seed(args.random_seed)
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•å’Œæ—¥å¿—ç›®å½•
    os.makedirs(args.model_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)
    
    # åœ¨æ—¶é—´æˆ³ç›®å½•ä¸‹åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„
    config_dir = os.path.join(args.model_dir, "config")
    summary_dir = os.path.join(args.model_dir, "summary")
    training_dir = os.path.join(args.model_dir, "training")
    evaluation_dir = os.path.join(args.model_dir, "evaluation")
    judge_logs_dir = os.path.join(args.model_dir, "judge_logs")
    logs_dir = os.path.join(args.model_dir, "logs")
    
    # åˆ›å»ºæ‰€æœ‰å­ç›®å½•
    os.makedirs(config_dir, exist_ok=True)
    os.makedirs(summary_dir, exist_ok=True)
    os.makedirs(training_dir, exist_ok=True)
    os.makedirs(evaluation_dir, exist_ok=True)
    os.makedirs(judge_logs_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    
    # åˆ›å»ºè¯„ä¼°å­ç›®å½•
    test_samples_dir = os.path.join(evaluation_dir, "test_samples")
    test_samples_successful_dir = os.path.join(test_samples_dir, "successful")
    test_samples_failed_dir = os.path.join(test_samples_dir, "failed")
    os.makedirs(test_samples_successful_dir, exist_ok=True)
    os.makedirs(test_samples_failed_dir, exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå­ç›®å½•
    checkpoints_dir = os.path.join(training_dir, "checkpoints")
    os.makedirs(checkpoints_dir, exist_ok=True)
    
    # åˆ›å»ºjudge_logså­ç›®å½•
    judge_logs_training_dir = os.path.join(judge_logs_dir, "training")
    judge_logs_test_dir = os.path.join(judge_logs_dir, "test")
    os.makedirs(judge_logs_training_dir, exist_ok=True)
    os.makedirs(judge_logs_test_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆä½¿ç”¨logså­ç›®å½•ï¼‰
    logger = HydraLogger(log_dir=logs_dir, results_dir=args.model_dir)
    
    logger.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒå¿«é€ŸRLæ”»å‡»æ™ºèƒ½ä½“ - Rainbow DQNï¼ˆæ”¯æŒæ•°æ®åˆ†å‰²ï¼‰")
    logger.logger.info(f"Benchmark: {args.benchmark}")
    logger.logger.info(f"å®éªŒç›®å½•: {args.model_dir}")
    logger.logger.info(f"æ—¥å¿—ç›®å½•: {logs_dir}")
    logger.logger.info("ğŸ“ å·²åˆ›å»ºç›®å½•ç»“æ„:")
    logger.logger.info(f"   - config/: {config_dir}")
    logger.logger.info(f"   - summary/: {summary_dir}")
    logger.logger.info(f"   - training/: {training_dir}")
    logger.logger.info(f"     - checkpoints/: {checkpoints_dir}")
    logger.logger.info(f"   - evaluation/: {evaluation_dir}")
    logger.logger.info(f"   - judge_logs/: {judge_logs_dir}")
    logger.logger.info(f"   - logs/: {logs_dir}")
    logger.logger.info(f"æ€»æ ·æœ¬æ•°: {args.max_samples}")
    logger.logger.info(f"æ•°æ®åˆ†å‰²: è®­ç»ƒ{args.train_ratio:.1%} / æµ‹è¯•{args.test_ratio:.1%} / éªŒè¯{args.validation_ratio:.1%}")
    logger.logger.info(f"è®­ç»ƒè½®æ•°: {args.episodes}")
    # è®¾ç½®æµ‹è¯•æ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨è®­ç»ƒæ—¶çš„å€¼ï¼‰
    max_queries_test = args.max_queries_test if args.max_queries_test is not None else args.max_queries
    logger.logger.info(f"è®­ç»ƒæ—¶æœ€å¤§æŸ¥è¯¢æ¬¡æ•°: {args.max_queries}")
    logger.logger.info(f"æµ‹è¯•æ—¶æœ€å¤§æŸ¥è¯¢æ¬¡æ•°: {max_queries_test}")
    
    # åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„åˆ†å‰²æ•°æ®ï¼‰
    train_examples, validation_examples = load_benchmark_data(args.benchmark, args.max_samples, args.random_seed, args.validation_ratio)
    if not train_examples:
        logger.logger.error("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
        return
    
    logger.logger.info(f"âœ… åŠ è½½äº† {len(train_examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    if validation_examples:
        logger.logger.info(f"âœ… åŠ è½½äº† {len(validation_examples)} ä¸ªéªŒè¯æ ·æœ¬")
    
    # åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆå¿…é¡»ä½¿ç”¨é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®ï¼‰
    test_file = f"data/split/{args.benchmark}_test.json"
    test_examples = []
    
    if not os.path.exists(test_file):
        logger.logger.error(f"âŒ é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_file}")
        logger.logger.error("è¯·å…ˆè¿è¡Œ prepare_data.py è„šæœ¬å‡†å¤‡æ•°æ®åˆ†å‰²")
        raise FileNotFoundError(f"é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_file}")
    
    # ä½¿ç”¨é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®
    with open(test_file, 'r') as f:
        test_data = json.load(f)
    
    for sample in test_data:
        example = PairwiseExample(
            question_id=sample["question_id"],
            instruction=sample["instruction"],
            response_a=sample["response_a"],
            response_b=sample["response_b"],
            model_a=sample["model_a"],
            model_b=sample["model_b"],
            metadata=sample.get("metadata", {})
        )
        test_examples.append(example)
    
    logger.logger.info(f"âœ… ä½¿ç”¨é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®: {len(test_examples)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ”»å‡»æ–¹æ³•
    attacks = get_attack_methods()
    logger.logger.info(f"âœ… åŠ è½½äº† {len(attacks)} ç§æ”»å‡»æ–¹æ³•")
    
    # åˆ›å»ºJudge
    try:
        if args.judge_type == "qwen":
            judge = create_qwen_judge(args.judge_model_path)
        elif args.judge_type == "llama":
            judge = create_llama_judge(args.judge_model_path)
        elif args.judge_type == "glm":
            judge = create_glm_judge(args.judge_model_path)
        elif args.judge_type == "mistral":
            judge = create_mistral_judge(args.judge_model_path)
        elif args.judge_type == "gemma":
            # Check if it's 1B model (text-only)
            model_path_lower = args.judge_model_path.lower()
            if "1b" in model_path_lower or "gemma-3-1b" in model_path_lower:
                judge = create_gemma_judge_1b(args.judge_model_path)
            else:
                judge = create_gemma_judge(args.judge_model_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„Judgeç±»å‹: {args.judge_type}")
        logger.logger.info("âœ… Judgeåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.logger.error(f"âŒ Judgeåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    train_env = EnhancedHydraAttackEnv(
        examples=train_examples,
        attacks=attacks,
        judge=judge,
        max_queries=args.max_queries,
        success_reward=args.success_reward,
        query_penalty=args.query_penalty,
        diversity_bonus=args.diversity_bonus,
        efficiency_bonus=args.efficiency_bonus,
        confidence_threshold=args.confidence_threshold
    )
    
    # åˆ›å»ºRainbow DQNæ™ºèƒ½ä½“
    state_dim = train_env.observation_space.shape[0]
    action_dim = train_env.action_space.n
    
    agent = RainbowDQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim,
        learning_rate=args.learning_rate,
        gamma=args.gamma,
        epsilon=args.epsilon,
        epsilon_min=args.epsilon_min,
        epsilon_decay=args.epsilon_decay,
        batch_size=args.batch_size,
        memory_size=args.memory_size,
        target_update_freq=args.target_update_freq,
        prioritized_replay=(args.prioritized_replay.lower() in ['true', '1', 'yes']),
        prioritized_replay_alpha=args.prioritized_replay_alpha,
        prioritized_replay_beta=args.prioritized_replay_beta,
        prioritized_replay_beta_increment=args.prioritized_replay_beta_increment,
        max_grad_norm=args.max_grad_norm
    )
    
    logger.logger.info(f"âœ… åˆ›å»ºRainbow DQNæ™ºèƒ½ä½“: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    logger.logger.info(f"   Rainbow DQNå‚æ•°: å­¦ä¹ ç‡={args.learning_rate}, æ‰¹æ¬¡å¤§å°={args.batch_size}, éšè—å±‚ç»´åº¦={args.hidden_dim}")
    logger.logger.info(f"   æ¢ç´¢å‚æ•°: Îµ={args.epsilon}, Îµ_min={args.epsilon_min}, Îµ_decay={args.epsilon_decay}")
    logger.logger.info(f"   ç½‘ç»œå‚æ•°: Î³={args.gamma}, å†…å­˜å¤§å°={args.memory_size}, ç›®æ ‡æ›´æ–°é¢‘ç‡={args.target_update_freq}")
    prioritized_replay_enabled = (args.prioritized_replay.lower() in ['true', '1', 'yes'])
    logger.logger.info(f"   ä¼˜å…ˆç»éªŒå›æ”¾: {prioritized_replay_enabled}, Î±={args.prioritized_replay_alpha}, Î²={args.prioritized_replay_beta}")
    logger.logger.info(f"   ç¯å¢ƒå‚æ•°: æˆåŠŸå¥–åŠ±={args.success_reward}, æŸ¥è¯¢æƒ©ç½š={args.query_penalty}, ç½®ä¿¡åº¦é˜ˆå€¼={args.confidence_threshold}")
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°ï¼ˆä½¿ç”¨æµ‹è¯•æ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°ï¼‰
    test_env = EnhancedHydraAttackEnv(
        examples=test_examples,
        attacks=attacks,
        judge=judge,
        max_queries=max_queries_test,
        success_reward=args.success_reward,
        query_penalty=args.query_penalty,
        diversity_bonus=args.diversity_bonus,
        efficiency_bonus=args.efficiency_bonus,
        confidence_threshold=args.confidence_threshold
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    config = RLConfig(
        algorithm="rainbowdqn",
        total_timesteps=args.episodes,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        gamma=args.gamma,
        epsilon_start=args.epsilon,
        epsilon_end=args.epsilon_min,
        epsilon_decay=args.epsilon_decay,
        target_update_freq=args.target_update_freq,
        save_freq=1000000,  # è®¾ç½®ä¸ºå¾ˆå¤§çš„å€¼ï¼Œå®é™…ä¸Šä¸ä¼šè§¦å‘ä¿å­˜
        eval_freq=args.eval_freq,
        max_queries=args.max_queries,  # è®­ç»ƒæ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°
        max_queries_test=max_queries_test,  # æµ‹è¯•æ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°ï¼ˆç”¨äºéªŒè¯é›†å’Œæµ‹è¯•é›†è¯„ä¼°ï¼‰
        success_reward=args.success_reward,
        query_penalty=args.query_penalty,
        diversity_bonus=args.diversity_bonus,
        efficiency_bonus=args.efficiency_bonus,
        confidence_threshold=args.confidence_threshold,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_min_delta=args.early_stopping_min_delta,
    )
    
    trainer = RLTrainer(
        env=train_env,
        agent=agent,
        config=config,
        save_dir=args.model_dir,
        test_env=test_env,  # ä¼ å…¥æµ‹è¯•ç¯å¢ƒç”¨äºè¯„ä¼°
        test_examples=test_examples,  # ä¼ å…¥æµ‹è¯•æ ·æœ¬ç”¨äºevaluate_on_unseen_data
        validation_examples=validation_examples,  # ä¼ å…¥éªŒè¯æ ·æœ¬ç”¨äºå‘¨æœŸæ€§éªŒè¯å’Œearly stopping
        attacks=attacks,  # ä¼ å…¥æ”»å‡»æ–¹æ³•
        judge=judge,  # ä¼ å…¥judge
        logger=logger  # ä¼ å…¥logger
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    training_results = trainer.train(args.episodes)
    
    training_time = time.time() - start_time
    logger.logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.logger.info(f"â±ï¸  è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆçœŸæ­£çš„æœªè§æ•°æ®ï¼‰
    logger.logger.info("ğŸ” åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆæœªè§æ•°æ®ï¼‰...")
    # å¯ç”¨è¯¦ç»†æ ·æœ¬è®°å½•ä¿å­˜ï¼ˆä½¿ç”¨æµ‹è¯•æ—¶çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°ï¼‰
    test_results = evaluate_on_unseen_data(agent, test_examples, attacks, judge, max_queries_test, 
                                         args.success_reward, args.query_penalty, args.diversity_bonus, 
                                         args.efficiency_bonus, args.confidence_threshold, logger,
                                         save_detailed_samples=True, save_dir=args.model_dir)
    
    # ä¿å­˜æ¨¡å‹ - ä¿å­˜åˆ°checkpointsç›®å½•ï¼ŒåŒæ—¶åœ¨æ ¹ç›®å½•ä¿ç•™ä¸€ä»½ï¼ˆå…¼å®¹æ€§ï¼‰
    model_path_checkpoint = os.path.join(checkpoints_dir, f"fast_rl_attacker_{args.benchmark}.pth")
    agent.save_model(model_path_checkpoint)
    logger.logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path_checkpoint}")
    
    # åŒæ—¶åœ¨æ ¹ç›®å½•ä¿ç•™ä¸€ä»½ï¼ˆå…¼å®¹æ€§ï¼‰
    model_path_root = os.path.join(args.model_dir, f"fast_rl_attacker_{args.benchmark}.pth")
    import shutil
    shutil.copy2(model_path_checkpoint, model_path_root)
    logger.logger.info(f"ğŸ’¾ æ¨¡å‹å·²å¤åˆ¶åˆ°æ ¹ç›®å½•: {model_path_root}")
    
    # ç§»åŠ¨best_modelå’Œfinal_modelåˆ°checkpointsç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    best_model_src = os.path.join(args.model_dir, "best_model.pth")
    final_model_src = os.path.join(args.model_dir, "final_model.pth")
    
    if os.path.exists(best_model_src):
        best_model_dst = os.path.join(checkpoints_dir, "best_model.pth")
        shutil.move(best_model_src, best_model_dst)
        logger.logger.info(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ç§»åŠ¨åˆ°: {best_model_dst}")
    
    if os.path.exists(final_model_src):
        final_model_dst = os.path.join(checkpoints_dir, "final_model.pth")
        shutil.move(final_model_src, final_model_dst)
        logger.logger.info(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ç§»åŠ¨åˆ°: {final_model_dst}")
    
    # ä¿å­˜åŠ¨ä½œæ˜ å°„ - ä¿å­˜åˆ°configç›®å½•ï¼ŒåŒæ—¶åœ¨æ ¹ç›®å½•ä¿ç•™ä¸€ä»½ï¼ˆå…¼å®¹æ€§ï¼‰
    action_mapping = {}
    for i, attack in enumerate(attacks):
        for j in range(attack.get_action_space_size()):
            action_mapping[i * 100 + j] = {
                'attack_method': attack.__class__.__name__,
                'action_id': j,
                'action_description': attack.get_action_description(j)
            }
    
    # ä¿å­˜åˆ°configç›®å½•
    mapping_path = os.path.join(config_dir, f"action_mapping_{args.benchmark}.json")
    with open(mapping_path, 'w') as f:
        json.dump(action_mapping, f, indent=2)
    logger.logger.info(f"ğŸ’¾ åŠ¨ä½œæ˜ å°„å·²ä¿å­˜åˆ°: {mapping_path}")
    
    # åŒæ—¶åœ¨æ ¹ç›®å½•ä¿ç•™ä¸€ä»½ï¼ˆå…¼å®¹æ€§ï¼‰
    mapping_path_root = os.path.join(args.model_dir, f"action_mapping_{args.benchmark}.json")
    with open(mapping_path_root, 'w') as f:
        json.dump(action_mapping, f, indent=2)
    
    # ä¿å­˜è®­ç»ƒé…ç½®å’Œç»“æœ - æ–‡ä»¶ååŒ…å«æ•°æ®é›†ä¿¡æ¯ï¼Œæ–°æ¨¡å‹ä¼šè¦†ç›–æ—§æ¨¡å‹
    config_dict = {
        'algorithm': 'RainbowDQN',
        'benchmark': args.benchmark,
        'max_samples': args.max_samples,
        'train_ratio': args.train_ratio,
        'test_ratio': args.test_ratio,
        'validation_ratio': args.validation_ratio,
        'episodes': args.episodes,
        'max_queries': args.max_queries,
        'max_queries_test': max_queries_test,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'random_seed': args.random_seed,
        'training_time': training_time,
        'train_samples': len(train_examples),
        'test_samples': len(test_examples),
        'validation_samples': len(validation_examples),
        'test_results': test_results,
        'judge_config': {
            'judge_type': args.judge_type,
            'judge_model_path': args.judge_model_path
        },
        'rainbowdqn_params': {
            'hidden_dim': args.hidden_dim,
            'gamma': args.gamma,
            'epsilon': args.epsilon,
            'epsilon_min': args.epsilon_min,
            'epsilon_decay': args.epsilon_decay,
            'memory_size': args.memory_size,
            'target_update_freq': args.target_update_freq,
            'prioritized_replay': (args.prioritized_replay.lower() in ['true', '1', 'yes']),
            'prioritized_replay_alpha': args.prioritized_replay_alpha,
            'prioritized_replay_beta': args.prioritized_replay_beta,
            'prioritized_replay_beta_increment': args.prioritized_replay_beta_increment,
            'max_grad_norm': args.max_grad_norm
        },
        'env_params': {
            'success_reward': args.success_reward,
            'query_penalty': args.query_penalty,
            'diversity_bonus': args.diversity_bonus,
            'efficiency_bonus': args.efficiency_bonus,
            'confidence_threshold': args.confidence_threshold
        },
        'training_config': {
            'eval_freq': args.eval_freq,
            'model_dir': args.model_dir,
            'log_dir': args.log_dir
        },
        'early_stopping_params': {
            'patience': args.early_stopping_patience,
            'min_delta': args.early_stopping_min_delta
        }
    }
    
    # ä¿å­˜è®­ç»ƒé…ç½® - ä¿å­˜åˆ°configç›®å½•ï¼ŒåŒæ—¶åœ¨æ ¹ç›®å½•ä¿ç•™ä¸€ä»½ï¼ˆå…¼å®¹æ€§ï¼‰
    config_dict = convert_numpy_types(config_dict)
    
    # ä¿å­˜åˆ°configç›®å½•
    config_path = os.path.join(config_dir, f"training_config_{args.benchmark}.json")
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=2)
    logger.logger.info(f"ğŸ’¾ è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # åŒæ—¶åœ¨æ ¹ç›®å½•ä¿ç•™ä¸€ä»½ï¼ˆå…¼å®¹æ€§ï¼‰
    config_path_root = os.path.join(args.model_dir, f"training_config_{args.benchmark}.json")
    with open(config_path_root, 'w') as f:
        json.dump(config_dict, f, indent=2)
    
    # ========== ä¿å­˜å®éªŒå…ƒæ•°æ® ==========
    import platform
    import socket
    experiment_metadata = {
        'experiment_id': os.path.basename(args.model_dir),
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
        'benchmark': args.benchmark,
        'algorithm': 'RainbowDQN',
        'environment': {
            'hostname': socket.gethostname(),
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'cuda_visible_devices': os.environ.get('CUDA_VISIBLE_DEVICES', 'N/A')
        },
        'data_info': {
            'train_samples': len(train_examples),
            'test_samples': len(test_examples),
            'validation_samples': len(validation_examples),
            'max_samples': args.max_samples,
            'train_ratio': args.train_ratio,
            'test_ratio': args.test_ratio,
            'validation_ratio': args.validation_ratio
        },
        'training_info': {
            'episodes': args.episodes,
            'training_time_seconds': training_time,
            'random_seed': args.random_seed
        }
    }
    experiment_metadata_path = os.path.join(config_dir, "experiment_metadata.json")
    with open(experiment_metadata_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_metadata, f, indent=2, ensure_ascii=False)
    logger.logger.info(f"ğŸ’¾ å®éªŒå…ƒæ•°æ®å·²ä¿å­˜åˆ°: {experiment_metadata_path}")
    
    # ========== ä¿å­˜Summaryæ–‡ä»¶ ==========
    
    # 1. ä¿å­˜è®­ç»ƒæ±‡æ€» (summary/training_summary.json)
    training_summary = {
        'total_episodes': args.episodes,
        'final_success_rate': training_results.get('final_eval', {}).get('success_rate', 0.0),
        'avg_episode_reward': np.mean(training_results.get('training_stats', {}).get('episode_rewards', [0])) if training_results.get('training_stats', {}).get('episode_rewards') else 0.0,
        'avg_queries_per_episode': np.mean(training_results.get('training_stats', {}).get('query_counts', [0])) if training_results.get('training_stats', {}).get('query_counts') else 0.0,
        'training_time': training_time,
        'best_checkpoint': 'best_model.pth' if os.path.exists(os.path.join(args.model_dir, 'best_model.pth')) else None,
        'early_stopping_triggered': False,  # å¯ä»¥ä»training_resultsä¸­è·å–
        'validation_scores': training_results.get('training_stats', {}).get('validation_scores', []),
        'episode_rewards': training_results.get('training_stats', {}).get('episode_rewards', []),
        'episode_success_rates': training_results.get('training_stats', {}).get('success_rates', []),
        # è®¡ç®—æ¯100ä¸ªepisodeçš„å¹³å‡å€¼ç”¨äºç»˜å›¾
        'episode_rewards_avg_100': [float(np.mean(training_results.get('training_stats', {}).get('episode_rewards', [])[i:i+100])) 
                                   for i in range(0, len(training_results.get('training_stats', {}).get('episode_rewards', [])), 100)
                                   if i < len(training_results.get('training_stats', {}).get('episode_rewards', []))],
        'episode_success_rates_avg_100': [float(np.mean(training_results.get('training_stats', {}).get('success_rates', [])[i:i+100])) 
                                         for i in range(0, len(training_results.get('training_stats', {}).get('success_rates', [])), 100)
                                         if i < len(training_results.get('training_stats', {}).get('success_rates', []))]
    }
    training_summary = convert_numpy_types(training_summary)
    training_summary_path = os.path.join(summary_dir, "training_summary.json")
    with open(training_summary_path, 'w', encoding='utf-8') as f:
        json.dump(training_summary, f, indent=2, ensure_ascii=False)
    logger.logger.info(f"ğŸ’¾ è®­ç»ƒæ±‡æ€»å·²ä¿å­˜åˆ°: {training_summary_path}")
    
    # 2. ä¿å­˜æµ‹è¯•æ±‡æ€» (summary/test_summary.json)
    test_summary = {
        'total_samples': test_results.get('total_samples', 0),
        'successful_attacks': test_results.get('successful_attacks', 0),
        'success_rate': test_results.get('success_rate', 0.0),
        'avg_queries': test_results.get('avg_queries', 0.0),
        'avg_queries_successful': test_results.get('avg_queries_successful', 0.0),
        'avg_reward': test_results.get('avg_reward', 0.0)
    }
    test_summary = convert_numpy_types(test_summary)
    test_summary_path = os.path.join(summary_dir, "test_summary.json")
    with open(test_summary_path, 'w', encoding='utf-8') as f:
        json.dump(test_summary, f, indent=2, ensure_ascii=False)
    logger.logger.info(f"ğŸ’¾ æµ‹è¯•æ±‡æ€»å·²ä¿å­˜åˆ°: {test_summary_path}")
    
    # 3. ä¿å­˜Episodeç»Ÿè®¡CSV (summary/episode_statistics.csv)
    import csv
    episode_stats_path = os.path.join(summary_dir, "episode_statistics.csv")
    training_stats = training_results.get('training_stats', {})
    episode_rewards = training_stats.get('episode_rewards', [])
    episode_lengths = training_stats.get('episode_lengths', [])
    episode_success_rates = training_stats.get('success_rates', [])
    episode_query_counts = training_stats.get('query_counts', [])
    episode_losses = training_stats.get('training_losses', [])
    episode_q_values = training_stats.get('q_values', [])
    episode_epsilon = training_stats.get('epsilon_values', [])
    
    with open(episode_stats_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['episode', 'reward', 'length', 'success_rate', 'query_count', 'loss', 'avg_q_value', 'epsilon'])
        max_len = max(len(episode_rewards), len(episode_lengths), len(episode_success_rates), 
                     len(episode_query_counts), len(episode_losses), len(episode_q_values), len(episode_epsilon))
        for i in range(max_len):
            writer.writerow([
                i + 1,
                episode_rewards[i] if i < len(episode_rewards) else 0,
                episode_lengths[i] if i < len(episode_lengths) else 0,
                episode_success_rates[i] if i < len(episode_success_rates) else 0,
                episode_query_counts[i] if i < len(episode_query_counts) else 0,
                episode_losses[i] if i < len(episode_losses) else 0,
                episode_q_values[i] if i < len(episode_q_values) else 0,
                episode_epsilon[i] if i < len(episode_epsilon) else 0
            ])
    logger.logger.info(f"ğŸ’¾ Episodeç»Ÿè®¡å·²ä¿å­˜åˆ°: {episode_stats_path}")
    
    # 4. ä¿å­˜æ”»å‡»ä½¿ç”¨ç»Ÿè®¡ (summary/attack_usage_stats.json)
    # ä»è®­ç»ƒç»“æœä¸­è·å–æ”»å‡»ä½¿ç”¨ç»Ÿè®¡
    attack_usage_stats_training = training_results.get('attack_usage_stats', {})
    
    # ä»æµ‹è¯•ç»“æœä¸­è·å–æ”»å‡»ä½¿ç”¨ç»Ÿè®¡
    attack_usage_stats_test = test_results.get('attack_usage_stats', {})
    
    attack_usage_stats = {
        'training': attack_usage_stats_training,
        'test': attack_usage_stats_test
    }
    attack_usage_stats = convert_numpy_types(attack_usage_stats)
    attack_usage_stats_path = os.path.join(summary_dir, "attack_usage_stats.json")
    with open(attack_usage_stats_path, 'w', encoding='utf-8') as f:
        json.dump(attack_usage_stats, f, indent=2, ensure_ascii=False)
    logger.logger.info(f"ğŸ’¾ æ”»å‡»ä½¿ç”¨ç»Ÿè®¡å·²ä¿å­˜åˆ°: {attack_usage_stats_path}")
    
    # åŒæ—¶ä¿å­˜è®­ç»ƒå’Œæµ‹è¯•çš„ç‹¬ç«‹æ–‡ä»¶ï¼ˆä»trainerä¿å­˜çš„æ–‡ä»¶å¤åˆ¶ï¼‰
    attack_usage_training_src = os.path.join(args.model_dir, "attack_usage_stats_training.json")
    attack_usage_training_dst = os.path.join(summary_dir, "attack_usage_stats_training.json")
    if os.path.exists(attack_usage_training_src):
        import shutil
        shutil.copy2(attack_usage_training_src, attack_usage_training_dst)
        logger.logger.info(f"ğŸ’¾ è®­ç»ƒæ”»å‡»ä½¿ç”¨ç»Ÿè®¡å·²å¤åˆ¶åˆ°: {attack_usage_training_dst}")
    
    attack_usage_test_src = os.path.join(args.model_dir, "attack_usage_stats_test.json")
    attack_usage_test_dst = os.path.join(summary_dir, "attack_usage_stats_test.json")
    if os.path.exists(attack_usage_test_src):
        import shutil
        shutil.copy2(attack_usage_test_src, attack_usage_test_dst)
        logger.logger.info(f"ğŸ’¾ æµ‹è¯•æ”»å‡»ä½¿ç”¨ç»Ÿè®¡å·²å¤åˆ¶åˆ°: {attack_usage_test_dst}")
    
    # ========== ä¿å­˜è®­ç»ƒæ›²çº¿æ•°æ® ==========
    training_stats = training_results.get('training_stats', {})
    training_curves = {
        'episode_rewards': episode_rewards,
        'episode_success_rates': episode_success_rates,
        'episode_query_counts': episode_query_counts,
        'validation_scores': training_stats.get('validation_scores', []),
        'validation_episodes': [i * args.eval_freq for i in range(len(training_stats.get('validation_scores', [])))],
        # æ·»åŠ è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡
        'training_losses': training_stats.get('training_losses', []),
        'q_values': training_stats.get('q_values', []),
        'epsilon_values': training_stats.get('epsilon_values', []),
        'max_q_values': training_stats.get('max_q_values', []),
        'min_q_values': training_stats.get('min_q_values', [])
    }
    training_curves = convert_numpy_types(training_curves)
    training_curves_path = os.path.join(training_dir, "training_curves.json")
    with open(training_curves_path, 'w', encoding='utf-8') as f:
        json.dump(training_curves, f, indent=2, ensure_ascii=False)
    logger.logger.info(f"ğŸ’¾ è®­ç»ƒæ›²çº¿æ•°æ®å·²ä¿å­˜åˆ°: {training_curves_path}")
    
    # ä¿å­˜trajectoryæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    trajectories_src = os.path.join(args.model_dir, "episode_trajectories.json")
    trajectories_dst = os.path.join(training_dir, "episode_trajectories.json")
    if os.path.exists(trajectories_src):
        import shutil
        shutil.copy2(trajectories_src, trajectories_dst)
        logger.logger.info(f"ğŸ’¾ Episode trajectorieså·²å¤åˆ¶åˆ°: {trajectories_dst}")
    
    # å°†training_stats.jsonç§»åŠ¨åˆ°summaryç›®å½•ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥ä¿ç•™åœ¨æ ¹ç›®å½•ï¼‰
    training_stats_src = os.path.join(args.model_dir, "training_stats.json")
    training_stats_dst = os.path.join(summary_dir, "training_stats.json")
    if os.path.exists(training_stats_src):
        import shutil
        shutil.copy2(training_stats_src, training_stats_dst)
        logger.logger.info(f"ğŸ’¾ è®­ç»ƒç»Ÿè®¡å·²å¤åˆ¶åˆ°: {training_stats_dst}")
    
    # åˆ›å»ºREADMEæ–‡ä»¶ï¼Œè¯´æ˜ç›®å½•ç»“æ„
    readme_path = os.path.join(args.model_dir, "README.md")
    readme_content = f"""# RLè®­ç»ƒå®éªŒç»“æœ

## å®éªŒä¿¡æ¯
- **Benchmark**: {args.benchmark}
- **ç®—æ³•**: Rainbow DQN
- **è®­ç»ƒæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}
- **è®­ç»ƒè€—æ—¶**: {training_time:.2f}ç§’
- **è®­ç»ƒè½®æ•°**: {args.episodes}
- **æ€»æ ·æœ¬æ•°**: {args.max_samples}

## ç›®å½•ç»“æ„è¯´æ˜

```
{os.path.basename(args.model_dir)}/
â”œâ”€â”€ config/                    # é…ç½®å’Œå…ƒæ•°æ®
â”‚   â”œâ”€â”€ training_config.json   # å®Œæ•´è¶…å‚æ•°é…ç½®
â”‚   â”œâ”€â”€ action_mapping.json    # åŠ¨ä½œæ˜ å°„
â”‚   â””â”€â”€ experiment_metadata.json  # å®éªŒå…ƒæ•°æ®ï¼ˆæ—¶é—´ã€ç¯å¢ƒç­‰ï¼‰
â”‚
â”œâ”€â”€ summary/                   # æ±‡æ€»æ•°æ®ï¼ˆå¿«é€ŸæŸ¥çœ‹ï¼‰
â”‚   â”œâ”€â”€ training_summary.json  # è®­ç»ƒè¿‡ç¨‹æ±‡æ€»
â”‚   â”œâ”€â”€ test_summary.json      # æµ‹è¯•ç»“æœæ±‡æ€»
â”‚   â”œâ”€â”€ attack_usage_stats.json  # æ”»å‡»æ–¹æ³•ä½¿ç”¨ç»Ÿè®¡
â”‚   â””â”€â”€ episode_statistics.csv   # Episodeçº§åˆ«ç»Ÿè®¡
â”‚
â”œâ”€â”€ training/                  # è®­ç»ƒè¿‡ç¨‹æ•°æ®
â”‚   â”œâ”€â”€ checkpoints/          # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”‚   â”œâ”€â”€ fast_rl_attacker_*.pth
â”‚   â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”‚   â””â”€â”€ final_model.pth
â”‚   â””â”€â”€ training_curves.json  # è®­ç»ƒæ›²çº¿æ•°æ®
â”‚
â”œâ”€â”€ evaluation/               # è¯„ä¼°æ•°æ®
â”‚   â””â”€â”€ test_samples/         # æµ‹è¯•æ ·æœ¬è¯¦ç»†è®°å½•
â”‚       â”œâ”€â”€ successful/       # æˆåŠŸæ”»å‡»æ ·æœ¬
â”‚       â”œâ”€â”€ failed/           # å¤±è´¥æ”»å‡»æ ·æœ¬
â”‚       â””â”€â”€ index.json        # æ ·æœ¬ç´¢å¼•
â”‚
â”œâ”€â”€ judge_logs/               # Judgeè¾“å…¥è¾“å‡ºè®°å½•
â”‚   â”œâ”€â”€ training/             # è®­ç»ƒé˜¶æ®µjudgeè®°å½•
â”‚   â””â”€â”€ test/                 # æµ‹è¯•é˜¶æ®µjudgeè®°å½•
â”‚
â”œâ”€â”€ logs/                     # æ—¥å¿—æ–‡ä»¶
â”‚
â”œâ”€â”€ fast_rl_attacker_{args.benchmark}.pth  # æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ action_mapping_{args.benchmark}.json  # åŠ¨ä½œæ˜ å°„ï¼ˆæ ¹ç›®å½•å¤‡ä»½ï¼‰
â””â”€â”€ training_config_{args.benchmark}.json # è®­ç»ƒé…ç½®ï¼ˆæ ¹ç›®å½•å¤‡ä»½ï¼‰
```

## å¿«é€ŸæŸ¥çœ‹ç»“æœ

### æŸ¥çœ‹æ±‡æ€»ç»“æœ
```bash
cat summary/training_summary.json
cat summary/test_summary.json
cat summary/attack_usage_stats.json
```

### æŸ¥çœ‹é…ç½®
```bash
cat config/training_config.json
```

### æŸ¥çœ‹æ—¥å¿—
```bash
cat logs/training.log
```

## å®éªŒç»“æœ

### è®­ç»ƒé›†è¡¨ç°
{training_results.get('final_eval', 'N/A')}

### æµ‹è¯•é›†è¡¨ç°ï¼ˆæœªè§æ•°æ®ï¼‰
- æˆåŠŸç‡: {test_results.get('success_rate', 0):.3f}
- å¹³å‡æŸ¥è¯¢æ¬¡æ•°: {test_results.get('avg_queries', 0):.3f}
- å¹³å‡æŸ¥è¯¢æ¬¡æ•°ï¼ˆæˆåŠŸï¼‰: {test_results.get('avg_queries_successful', 0):.3f}
- å¹³å‡å¥–åŠ±: {test_results.get('avg_reward', 0):.3f}
"""
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    logger.logger.info(f"ğŸ“ READMEå·²ä¿å­˜åˆ°: {readme_path}")
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    logger.logger.info("=" * 80)
    logger.logger.info("ğŸ“Š æœ€ç»ˆè®­ç»ƒç»“æœ")
    logger.logger.info("=" * 80)
    logger.logger.info(f"è®­ç»ƒé›†è¡¨ç°: {training_results['final_eval']}")
    logger.logger.info(f"æµ‹è¯•é›†è¡¨ç°ï¼ˆæœªè§æ•°æ®ï¼‰: {test_results}")
    logger.logger.info("=" * 80)
    logger.logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åœ¨: {args.model_dir}")
    logger.logger.info(f"ğŸ“ æŸ¥çœ‹READMEäº†è§£ç›®å½•ç»“æ„: {readme_path}")
    
    return {
        'training_results': training_results,
        'test_results': test_results,
        'config': config_dict
    }


if __name__ == "__main__":
    main()

