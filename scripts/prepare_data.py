#!/usr/bin/env python3
"""
æ•°æ®å‡†å¤‡è„šæœ¬
ç¡®ä¿æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ†å‰²
"""

import argparse
import json
import os
import random
from pathlib import Path
from typing import List, Dict, Any


def prepare_benchmark_data(benchmark: str, max_samples, random_seed: int = 42, train_ratio: float = 0.8, test_ratio: float = 0.2) -> None:
    """å‡†å¤‡benchmarkæ•°æ®ï¼Œç¡®ä¿æ•°æ®åˆ†å‰²ä¸€è‡´æ€§"""
    
    # å¯¹äº llmbar å’Œ mtbenchï¼Œæ•°æ®å·²ç»å­˜åœ¨äº split ç›®å½•ä¸­ï¼Œç›´æ¥è·³è¿‡å¤„ç†
    if benchmark in ["llmbar", "mtbench"]:
        train_file = f"data/split/{benchmark}_train.json"
        test_file = f"data/split/{benchmark}_test.json"
        if os.path.exists(train_file) and os.path.exists(test_file):
            print(f"âœ… {benchmark} æ•°æ®å·²å­˜åœ¨äº split ç›®å½•ä¸­ï¼Œè·³è¿‡å¤„ç†")
            return
        else:
            print(f"âš ï¸  {benchmark} æ•°æ®æ–‡ä»¶ä¸å®Œæ•´ï¼Œéœ€è¦é‡æ–°å¤„ç†")
    
    data_file = f"data/processed/{benchmark}_processed.json"
    
    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return
    
    # åŠ è½½åŸå§‹æ•°æ®
    with open(data_file, 'r') as f:
        data = json.load(f)
    
    if not data:
        print(f"âŒ {benchmark} æ•°æ®ä¸ºç©º")
        return
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®æ ·æœ¬æ•°: {len(data)}")
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
    random.seed(random_seed)
    
    # éšæœºæ‰“ä¹±æ•°æ®
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    
    # é€‰æ‹©æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    if max_samples == "full" or max_samples is None:
        selected_data = shuffled_data
        print(f"ğŸ“Š ä½¿ç”¨å…¨é‡æ•°æ®: {len(selected_data)} æ ·æœ¬")
    else:
        selected_data = shuffled_data[:max_samples]
        print(f"ğŸ“Š é€‰æ‹©æ ·æœ¬æ•°: {len(selected_data)}")
    
    # è®¡ç®—åˆ†å‰²ç‚¹
    val_ratio = 0.0
    
    train_end = int(len(selected_data) * train_ratio)
    
    train_data = selected_data[:train_end]
    test_data = selected_data[train_end:]
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"  - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ ({len(train_data)/len(selected_data)*100:.1f}%)")
    print(f"  - æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬ ({len(test_data)/len(selected_data)*100:.1f}%)")
    
    # ä¿å­˜åˆ†å‰²åçš„æ•°æ®
    os.makedirs("data/split", exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé›†
    train_file = f"data/split/{benchmark}_train.json"
    with open(train_file, 'w') as f:
        json.dump(train_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_file}")
    
    
    # ä¿å­˜æµ‹è¯•é›†
    test_file = f"data/split/{benchmark}_test.json"
    with open(test_file, 'w') as f:
        json.dump(test_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
    
    # ä¿å­˜æ•°æ®åˆ†å‰²ä¿¡æ¯
    split_info = {
        "benchmark": benchmark,
        "total_samples": len(selected_data),
        "train_samples": len(train_data),
        "test_samples": len(test_data),
        "train_ratio": train_ratio,
        "test_ratio": test_ratio,
        "random_seed": random_seed,
        "data_files": {
            "train": train_file,
            "test": test_file
        }
    }
    
    split_info_file = f"data/split/{benchmark}_split_info.json"
    with open(split_info_file, 'w') as f:
        json.dump(split_info, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ åˆ†å‰²ä¿¡æ¯å·²ä¿å­˜åˆ°: {split_info_file}")
    
    print(f"âœ… {benchmark} æ•°æ®å‡†å¤‡å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å‡†å¤‡æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ†å‰²")
    parser.add_argument("--benchmark", type=str, default="alpaca_eval", 
                       choices=["alpaca_eval", "arena_hard", "code_judge_bench", "llmbar", "mtbench"],
                       help="æµ‹è¯•çš„benchmark")
    parser.add_argument("--max_samples", default=200, help="æœ€å¤§æ ·æœ¬æ•°ï¼Œä½¿ç”¨ 'full' è¡¨ç¤ºä½¿ç”¨å…¨é‡æ•°æ®")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--test_ratio", type=float, default=0.2, help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--random_seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    print("ğŸš€ æ•°æ®å‡†å¤‡ï¼šç¡®ä¿æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ†å‰²")
    print("=" * 60)
    print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"  - Benchmark: {args.benchmark}")
    if args.max_samples == "full":
        print(f"  - æ•°æ®æ¨¡å¼: å…¨é‡æ•°æ®")
    else:
        print(f"  - æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print(f"  - è®­ç»ƒé›†æ¯”ä¾‹: {args.train_ratio}")
    print(f"  - æµ‹è¯•é›†æ¯”ä¾‹: {args.test_ratio}")
    print(f"  - éšæœºç§å­: {args.random_seed}")
    print("")
    
    prepare_benchmark_data(args.benchmark, args.max_samples, args.random_seed, args.train_ratio, args.test_ratio)
    
    print("")
    print("ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - è®­ç»ƒé›†: data/split/{args.benchmark}_train.json")
    print(f"  - æµ‹è¯•é›†: data/split/{args.benchmark}_test.json")
    print(f"  - åˆ†å‰²ä¿¡æ¯: data/split/{args.benchmark}_split_info.json")
    print("")
    print("ğŸ’¡ æ‰€æœ‰æ–¹æ³•ç°åœ¨å°†ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•é›†è¿›è¡Œå…¬å¹³å¯¹æ¯”ï¼")


if __name__ == "__main__":
    main()
