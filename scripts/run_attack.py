#!/usr/bin/env python3
"""
Hydra-Attack ä¸»æµ‹è¯•è„šæœ¬
ç»Ÿä¸€è¿è¡Œæ‰€æœ‰æ”»å‡»æµ‹è¯•
"""

import argparse
import sys
import json
import time
import os
import random
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

from data_types import PairwiseExample, AttackResult, BenchmarkType, AttackType
from attacks import (
    FlipAttackFWO, FlipAttackFCW, FlipAttackFCS, UncertaintyAttack, PositionAttack, DistractorAttack,
    PromptInjectionAttack, MarkerInjectionAttack, FormattingAttack,
    AuthorityAttack, UnicodeAttack, CoTPoisoningAttack, EmojiAttack
)
from evaluation.qwen_judge import create_qwen_judge
from evaluation.llama_judge import create_llama_judge
from evaluation.glm_judge import create_glm_judge
from evaluation.mistral_judge import create_mistral_judge
from evaluation.gemma_judge import create_gemma_judge
from evaluation.gemma_judge1b import create_gemma_judge as create_gemma_judge_1b
from utils.logger import HydraLogger, AttackSample, AttackResults, BenchmarkResults, ExperimentConfig


def load_benchmark_data(benchmark: str, max_samples = 10, random_seed: int = 42) -> List[PairwiseExample]:
    """åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆç¡®ä¿ä¸å…¶ä»–æ–¹æ³•ä½¿ç”¨ç›¸åŒæ•°æ®ï¼‰"""
    # ä¼˜å…ˆä½¿ç”¨åˆ†å‰²åçš„æµ‹è¯•æ•°æ®
    test_file = f"data/split/{benchmark}_test.json"
    
    if os.path.exists(test_file):
        # ä½¿ç”¨é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®
        with open(test_file, 'r') as f:
            data = json.load(f)
        print(f"âœ… ä½¿ç”¨é¢„åˆ†å‰²çš„æµ‹è¯•æ•°æ®: {len(data)} æ ·æœ¬")
    else:
        raise ValueError(f"âŒ {benchmark} æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
    
    examples = []
    for sample in data:
        example = PairwiseExample(
            question_id=sample["question_id"],
            instruction=sample["instruction"],
            response_a=sample["response_a"],
            response_b=sample["response_b"],
            model_a=sample["model_a"],
            model_b=sample["model_b"]
        )
        examples.append(example)
    
    return examples


def get_attack_methods(attack_methods: List[str]) -> Dict[str, Any]:
    """è·å–æ”»å‡»æ–¹æ³•å®ä¾‹"""
    all_attacks = {
        "flip_attack_fwo": FlipAttackFWO(),
        "flip_attack_fcw": FlipAttackFCW(),
        "flip_attack_fcs": FlipAttackFCS(),
        "uncertainty_attack": UncertaintyAttack(),
        "position_attack": PositionAttack(),
        "distractor_attack": DistractorAttack(),
        "prompt_injection_attack": PromptInjectionAttack(),
        "marker_injection_attack": MarkerInjectionAttack(),
        "formatting_attack": FormattingAttack(),
        "authority_attack": AuthorityAttack(),
        "unicode_attack": UnicodeAttack(),
        "cot_poisoning_attack": CoTPoisoningAttack(),
        "emoji_attack": EmojiAttack(),
    }
    
    if "all" in attack_methods:
        return all_attacks
    
    selected_attacks = {}
    for method in attack_methods:
        if method in all_attacks:
            selected_attacks[method] = all_attacks[method]
        else:
            print(f"âš ï¸  æœªçŸ¥çš„æ”»å‡»æ–¹æ³•: {method}")
    
    return selected_attacks


def test_attacks(examples: List[PairwiseExample], judge, attacks: Dict[str, Any], 
                max_samples = 10, logger: HydraLogger = None, max_queries: int = None, 
                results_dir: str = None, benchmark: str = None,
                experiment_config: Dict[str, Any] = None) -> Dict[str, AttackResults]:
    """æµ‹è¯•æ”»å‡»æ–¹æ³•"""
    # if logger:
    #     logger.log_benchmark_start("æ”»å‡»æ–¹æ³•æµ‹è¯•", min(len(examples), max_samples))
    
    results = {}
    
    # æ·»åŠ æ”»å‡»æ–¹æ³•è¿›åº¦æ¡
    attack_progress = tqdm(
        attacks.items(), 
        desc="æµ‹è¯•æ”»å‡»æ–¹æ³•",
        unit="æ”»å‡»",
        ncols=100,
        position=1,
        leave=False
    )
    
    for attack_name, attack in attack_progress:
        # å¤„ç†æ‰€æœ‰æ”»å‡»æ–¹æ³•ï¼ˆç°åœ¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰
        attack_progress.set_description(f"æµ‹è¯• {attack_name}")
        if logger:
            logger.log_attack_start(attack_name, attack.get_action_space_size())
        
        attack_results = test_single_attack(
            examples, judge, attack, max_samples, attack_name, logger, max_queries
        )
        results[attack_name] = attack_results
        
        if logger:
            logger.log_attack_results(attack_results)
        
        # ç«‹å³ä¿å­˜å•ä¸ªæ”»å‡»æ–¹æ³•çš„ç»“æœæ–‡ä»¶
        if results_dir and benchmark:
            save_single_attack_result(attack_results, results_dir, benchmark, experiment_config)
    
    return results


def test_single_attack(examples: List[PairwiseExample], judge, attack, max_samples, 
                      attack_name: str, logger: HydraLogger = None, max_queries: int = None) -> AttackResults:
    """æµ‹è¯•å•ä¸ªæ”»å‡»æ–¹æ³•"""
    successful_attacks = 0
    total_samples = 0
    samples = []
    confidence_changes = []
    total_queries = 0  # All attacks (success + failure)
    total_queries_successful = 0  # Only successful attacks
    total_time = 0.0
    efficiency_scores = []
    
    # æ·»åŠ è¿›åº¦æ¡
    if max_samples == "full" or max_samples is None:
        sample_limit = len(examples)
    else:
        sample_limit = min(max_samples, len(examples))
    
    progress_bar = tqdm(
        enumerate(examples[:sample_limit]), 
        total=sample_limit,
        desc=f"æµ‹è¯• {attack_name}",
        unit="æ ·æœ¬",
        ncols=100,
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
    )
    
    for i, example in progress_bar:
        sample_id = f"{attack_name}_sample_{i+1}"
        
        try:
            # è·å–åŸå§‹åå¥½
            original_prompt = judge.get_judge_prompt(example)
            original_response = judge.judge_pairwise(example)
            original_preference = "A" if original_response.preference == 0 else "B"
            original_confidence = original_response.confidence
            original_raw_response = original_response.raw_response or "N/A"

            # è®°å½•åŸå§‹judgeè¾“å…¥è¾“å‡º
            if logger:
                logger.logger.info(f"    ğŸ” [åŸå§‹Judge] Input Prompt:")
                # æŒ‰è¡Œè¾“å‡ºpromptï¼Œä¾¿äºé˜…è¯»
                for line in original_prompt.split('\n'):
                    logger.logger.info(f"        {line}")
                logger.logger.info(f"    ğŸ” [åŸå§‹Judge] Raw Output:")
                # æŒ‰è¡Œè¾“å‡ºresponseï¼Œä¾¿äºé˜…è¯»
                for line in original_raw_response.split('\n'):
                    logger.logger.info(f"        {line}")
                logger.logger.info(f"    ğŸ” [åŸå§‹Judge] Parsed: {original_preference} (confidence: {original_confidence:.3f})")

            # ç¡®å®šç›®æ ‡åå¥½
            target_preference = 1 - original_response.preference
            
            # å°è¯•æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œé€‰æ‹©æœ€æˆåŠŸçš„
            best_success = False
            best_modified_a = example.response_a
            best_modified_b = example.response_b
            best_modified_instruction = None  # æ·»åŠ ä¿®æ”¹åæŒ‡ä»¤çš„è·Ÿè¸ª
            best_action = "no_action"
            best_new_preference = original_preference
            best_new_confidence = original_confidence
            queries_used = 0
            attack_start_time = time.time()
            
            # ç¡®å®šæœ€å¤§æŸ¥è¯¢æ¬¡æ•°
            if max_queries is not None:
                max_actions = min(attack.get_action_space_size(), max_queries)
            else:
                max_actions = attack.get_action_space_size()
            
            for action in range(max_actions):
                try: 
                    modified_a, modified_b = attack.apply_action(example, action, target_preference)
                    queries_used += 1  # æ¯æ¬¡å°è¯•éƒ½ç®—ä¸€æ¬¡æŸ¥è¯¢
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯FlipAttackã€promptinjectionï¼ˆä¿®æ”¹instructionçš„attackï¼‰
                    if hasattr(attack, 'get_modified_instruction'):
                        # FlipAttack: ä½¿ç”¨ä¿®æ”¹åçš„instruction
                        modified_instruction = attack.get_modified_instruction(example)
                        # åˆ›å»ºä¸´æ—¶exampleç”¨äºè·å–prompt
                        temp_example = PairwiseExample(
                            question_id=example.question_id,
                            instruction=modified_instruction,
                            response_a=example.response_a,
                            response_b=example.response_b,
                            model_a=example.model_a,
                            model_b=example.model_b,
                            metadata=example.metadata
                        )
                        new_prompt = judge.get_judge_prompt(temp_example)
                        new_response = judge.judge_pairwise(example, modified_instruction)
                    else:
                        # å…¶ä»–attack: åˆ›å»ºä¿®æ”¹åçš„æ ·æœ¬
                        modified_instruction = None  # å…¶ä»–æ”»å‡»ä¸ä¿®æ”¹instruction
                        modified_example = PairwiseExample(
                            question_id=example.question_id,
                            instruction=example.instruction,
                            response_a=modified_a,
                            response_b=modified_b,
                            model_a=example.model_a,
                            model_b=example.model_b,
                            metadata=example.metadata
                        )
                        new_prompt = judge.get_judge_prompt(modified_example)
                        new_response = judge.judge_pairwise(modified_example)
                    
                    new_preference = "A" if new_response.preference == 0 else "B"
                    new_confidence = new_response.confidence
                    new_raw_response = new_response.raw_response or "N/A"
                    
                    # è®°å½•æ”»å‡»åjudgeè¾“å…¥è¾“å‡º
                    if logger:
                        logger.logger.info(f"    ğŸ” [æ”»å‡»JudgeæŸ¥è¯¢#{queries_used}] Action: {attack.get_action_description(action)}")
                        logger.logger.info(f"    ğŸ” [æ”»å‡»JudgeæŸ¥è¯¢#{queries_used}] Input Prompt:")
                        # æŒ‰è¡Œè¾“å‡ºpromptï¼Œä¾¿äºé˜…è¯»
                        for line in new_prompt.split('\n'):
                            logger.logger.info(f"        {line}")
                        logger.logger.info(f"    ğŸ” [æ”»å‡»JudgeæŸ¥è¯¢#{queries_used}] Raw Output:")
                        # æŒ‰è¡Œè¾“å‡ºresponseï¼Œä¾¿äºé˜…è¯»
                        for line in new_raw_response.split('\n'):
                            logger.logger.info(f"        {line}")
                        logger.logger.info(f"    ğŸ” [æ”»å‡»JudgeæŸ¥è¯¢#{queries_used}] Parsed: {new_preference} (confidence: {new_confidence:.3f})")
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ - ä½¿ç”¨æ”»å‡»æ–¹æ³•çš„æˆåŠŸåˆ¤æ–­é€»è¾‘
                    # å¯¹äºPositionAttackï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆå› ä¸ºäº¤æ¢äº†ä½ç½®ï¼‰
                    original_pref_int = 0 if original_preference == "A" else 1
                    new_pref_int = 0 if new_preference == "A" else 1
                    success = attack.is_attack_successful(original_pref_int, new_pref_int)
                    if success:
                        best_success = True
                        best_modified_a = modified_a
                        best_modified_b = modified_b
                        best_modified_instruction = modified_instruction  # ä¿å­˜ä¿®æ”¹åçš„æŒ‡ä»¤
                        best_action = attack.get_action_description(action)
                        best_new_preference = new_preference
                        best_new_confidence = new_confidence
                        break  # æ‰¾åˆ°æˆåŠŸçš„æ”»å‡»å°±åœæ­¢
                        
                except Exception as e:
                    queries_used += 1  # å³ä½¿å¤±è´¥ä¹Ÿç®—ä¸€æ¬¡æŸ¥è¯¢
                    if logger:
                        logger.logger.warning(f"    âš ï¸  [æ”»å‡»JudgeæŸ¥è¯¢#{queries_used}] æŸ¥è¯¢å¤±è´¥: {e}")
                    continue
            
            if best_success:
                successful_attacks += 1
                total_queries_successful += queries_used  # Only count queries for successful attacks
            
            total_samples += 1
            confidence_change = best_new_confidence - original_confidence
            confidence_changes.append(confidence_change)
            
            # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
            attack_time = time.time() - attack_start_time
            efficiency_score = 1.0 / queries_used if best_success else 0.0
            
            # æ›´æ–°ç»Ÿè®¡
            total_queries += queries_used  # Count queries for all attacks
            total_time += attack_time
            efficiency_scores.append(efficiency_score)
            
            # åˆ›å»ºæ”»å‡»æ ·æœ¬è®°å½•
            sample = AttackSample(
                sample_id=sample_id,
                question_id=example.question_id,
                instruction=example.instruction,
                response_a=example.response_a,
                response_b=example.response_b,
                model_a=example.model_a,
                model_b=example.model_b,
                original_preference=original_preference,
                original_confidence=original_confidence,
                attack_method=attack_name,
                attack_action=best_action,
                modified_instruction=best_modified_instruction,  # æ·»åŠ ä¿®æ”¹åçš„æŒ‡ä»¤
                modified_response_a=best_modified_a,
                modified_response_b=best_modified_b,
                new_preference=best_new_preference,
                new_confidence=best_new_confidence,
                success=best_success,
                timestamp=time.time(),
                queries_used=queries_used,
                efficiency_score=efficiency_score,
                attack_time=attack_time,
                metadata=example.metadata
            )
            samples.append(sample)
            
            # è®°å½•åˆ°æ—¥å¿—
            if logger:
                logger.log_attack_sample(sample)
            
        except Exception as e:
            if logger:
                logger.logger.error(f"æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
            continue
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦å˜åŒ–
    avg_confidence_change = sum(confidence_changes) / len(confidence_changes) if confidence_changes else 0.0
    
    # è®¡ç®—æ•ˆç‡ç»Ÿè®¡
    avg_queries_used = total_queries / total_samples if total_samples > 0 else 0.0  # AQA: Average Queries per Attack (all attempts)
    avg_queries_successful = total_queries_successful / successful_attacks if successful_attacks > 0 else 0.0  # AQSA: Average Queries per Successful Attack
    avg_efficiency_score = sum(efficiency_scores) / len(efficiency_scores) if efficiency_scores else 0.0
    avg_attack_time = total_time / total_samples if total_samples > 0 else 0.0
    total_queries_saved = total_samples * attack.get_action_space_size() - total_queries
    
    # åˆ›å»ºæ”»å‡»ç»“æœ
    success_rate = successful_attacks / total_samples if total_samples > 0 else 0.0
    success_rate = round(success_rate, 4)  # ä¿ç•™å››ä½å°æ•°ç²¾åº¦
    attack_results = AttackResults(
        attack_method=attack_name,
        total_samples=total_samples,
        successful_attacks=successful_attacks,
        success_rate=success_rate,
        action_space_size=attack.get_action_space_size(),
        avg_confidence_change=avg_confidence_change,
        avg_queries_used=avg_queries_used,  # AQA: Average Queries per Attack (all attempts)
        avg_queries_successful=avg_queries_successful,  # AQSA: Average Queries per Successful Attack
        avg_efficiency_score=avg_efficiency_score,
        avg_attack_time=avg_attack_time,
        total_queries_saved=total_queries_saved,
        samples=samples
    )
    
    return attack_results


def save_single_attack_result(attack_results: AttackResults, results_dir: str, benchmark: str, 
                               experiment_config: Dict[str, Any] = None):
    """ç«‹å³ä¿å­˜å•ä¸ªæ”»å‡»æ–¹æ³•çš„ç»“æœæ–‡ä»¶"""
    baseline_file = os.path.join(results_dir, f"baseline_{attack_results.attack_method}_results.json")
    
    # åŸºç¡€ç»“æœæ•°æ®
    # ç¡®ä¿success_rateä¿ç•™å››ä½å°æ•°ç²¾åº¦
    success_rate = round(attack_results.success_rate, 4)
    baseline_data = {
        "benchmark": benchmark,
        "attack_method": attack_results.attack_method,
        "timestamp": time.strftime("%Y%m%d_%H%M%S"),
        "total_samples": attack_results.total_samples,
        "successful_attacks": attack_results.successful_attacks,
        "success_rate": success_rate,
        "action_space_size": attack_results.action_space_size,
        "avg_confidence_change": attack_results.avg_confidence_change,
        "avg_queries_used": attack_results.avg_queries_used,  # AQA: Average Queries per Attack (all attempts)
        "avg_queries_successful": attack_results.avg_queries_successful,  # AQSA: Average Queries per Successful Attack
        "avg_efficiency_score": attack_results.avg_efficiency_score,
        "avg_attack_time": attack_results.avg_attack_time,
        "total_queries_saved": attack_results.total_queries_saved
    }
    
    # æ·»åŠ å®éªŒé…ç½®ä¿¡æ¯ï¼ˆè¶…å‚æ•°ï¼‰
    if experiment_config:
        baseline_data["experiment_config"] = experiment_config
    else:
        # å¦‚æœæ²¡æœ‰ä¼ å…¥é…ç½®ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡å’Œé»˜è®¤å€¼è·å–
        baseline_data["experiment_config"] = {
            "cuda_visible_devices": os.environ.get("CUDA_VISIBLE_DEVICES", "unknown"),
            "max_samples": "unknown",
            "judge_model_path": "unknown",
            "judge_type": "unknown",
            "random_seed": "unknown",
            "baseline_max_queries": "unknown"
        }
    
    with open(baseline_file, 'w') as f:
        json.dump(baseline_data, f, indent=2)
    
    print(f"ğŸ’¾ {attack_results.attack_method} ç»“æœå·²ç«‹å³ä¿å­˜åˆ°: {baseline_file}")
    # åŒæ—¶è®°å½•åˆ°æ—¥å¿—ä¸­
    import logging
    logger = logging.getLogger('hydra_attack')
    logger.info(f"ğŸ’¾ {attack_results.attack_method} ç»“æœå·²ç«‹å³ä¿å­˜åˆ°: {baseline_file}")


"""
def test_rl_combined_attacks(examples: List[PairwiseExample], judge, attacks: Dict[str, Any], 
                            max_samples = 10, logger: HydraLogger = None, max_queries: int = None) -> AttackResults:
    '''æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç»„åˆæ”»å‡»'''
    # å¤„ç†max_sampleså‚æ•°ï¼Œæ”¯æŒ"full"å­—ç¬¦ä¸²
    if isinstance(max_samples, str) and max_samples.lower() == "full":
        actual_max_samples = len(examples)
    else:
        actual_max_samples = min(len(examples), int(max_samples))
    
    if logger:
        logger.log_benchmark_start("å¼ºåŒ–å­¦ä¹ ç»„åˆæ”»å‡»", actual_max_samples)
    
    # æ”¶é›†æ‰€æœ‰æ”»å‡»æ–¹æ³•å®ä¾‹
    all_attack_instances = []
    for attack_name, attack in attacks.items():
        if isinstance(attack, dict):
            for mode, attack_instance in attack.items():
                all_attack_instances.append(attack_instance)
        else:
            all_attack_instances.append(attack)
    
    successful_attacks = 0
    total_samples = 0
    samples = []
    confidence_changes = []
    total_action_space = sum(attack.get_action_space_size() for attack in all_attack_instances)
    total_queries = 0
    total_time = 0.0
    efficiency_scores = []
    
    # æ·»åŠ è¿›åº¦æ¡
    sample_limit = actual_max_samples
    
    progress_bar = tqdm(
        enumerate(examples[:sample_limit]), 
        total=sample_limit,
        desc="æµ‹è¯• RLç»„åˆæ”»å‡»",
        unit="æ ·æœ¬",
        ncols=100,
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
    )
    
    for i, example in progress_bar:
        sample_id = f"rl_combined_sample_{i+1}"
        
        try:
            # è·å–åŸå§‹åå¥½
            original_response = judge.judge_pairwise(example)
            original_preference = "A" if original_response.preference == 0 else "B"
            original_confidence = original_response.confidence
            
            # å°è¯•æ‰€æœ‰æ”»å‡»æ–¹æ³•ï¼Œé€‰æ‹©æœ€æˆåŠŸçš„
            best_success = False
            best_attack_name = "no_attack"
            best_modified_a = example.response_a
            best_modified_b = example.response_b
            best_modified_instruction = None  # æ·»åŠ ä¿®æ”¹åæŒ‡ä»¤çš„è·Ÿè¸ª
            best_action = "no_action"
            best_new_preference = original_preference
            best_new_confidence = original_confidence
            queries_used = 0
            attack_start_time = time.time()
            
            for attack in all_attack_instances:
                try:
                    # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æŸ¥è¯¢æ¬¡æ•°é™åˆ¶
                    if max_queries is not None and queries_used >= max_queries:
                        break
                    
                    # å°è¯•æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œä½†é™åˆ¶æŸ¥è¯¢æ¬¡æ•°
                    if max_queries is not None:
                        max_actions = min(attack.get_action_space_size(), max_queries - queries_used)
                    else:
                        max_actions = attack.get_action_space_size()
                    
                    for action in range(max_actions):
                        modified_a, modified_b = attack.apply_action(example, action)
                        queries_used += 1  # æ¯æ¬¡å°è¯•éƒ½ç®—ä¸€æ¬¡æŸ¥è¯¢
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯FlipAttackï¼ˆä¿®æ”¹instructionçš„attackï¼‰
                        if hasattr(attack, 'get_modified_instruction'):
                            # FlipAttack: ä½¿ç”¨ä¿®æ”¹åçš„instruction
                            modified_instruction = attack.get_modified_instruction(example)
                            new_response = judge.judge_pairwise(example, modified_instruction)
                        else:
                            # å…¶ä»–attack: åˆ›å»ºä¿®æ”¹åçš„æ ·æœ¬
                            modified_instruction = None  # å…¶ä»–æ”»å‡»ä¸ä¿®æ”¹instruction
                            modified_example = PairwiseExample(
                                question_id=example.question_id,
                                instruction=example.instruction,
                                response_a=modified_a,
                                response_b=modified_b,
                                model_a=example.model_a,
                                model_b=example.model_b,
                                metadata=example.metadata
                            )
                            new_response = judge.judge_pairwise(modified_example)
                        
                        new_preference = "A" if new_response.preference == 0 else "B"
                        new_confidence = new_response.confidence
                        
                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ - ä½¿ç”¨æ”»å‡»æ–¹æ³•çš„æˆåŠŸåˆ¤æ–­é€»è¾‘
                        # å¯¹äºPositionAttackï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆå› ä¸ºäº¤æ¢äº†ä½ç½®ï¼‰
                        original_pref_int = 0 if original_preference == "A" else 1
                        new_pref_int = 0 if new_preference == "A" else 1
                        if attack.is_attack_successful(original_pref_int, new_pref_int):
                            best_success = True
                            best_attack_name = f"{attack.attack_type.value}_{action}"
                            best_modified_a = modified_a
                            best_modified_b = modified_b
                            best_modified_instruction = modified_instruction  # ä¿å­˜ä¿®æ”¹åçš„æŒ‡ä»¤
                            best_action = attack.get_action_description(action)
                            best_new_preference = new_preference
                            best_new_confidence = new_confidence
                            break
                    
                    if best_success:
                        break
                        
                except Exception as e:
                    continue
            
            if best_success:
                successful_attacks += 1
            
            total_samples += 1
            confidence_change = best_new_confidence - original_confidence
            confidence_changes.append(confidence_change)
            
            # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
            attack_time = time.time() - attack_start_time
            efficiency_score = 1.0 / queries_used if best_success else 0.0
            
            # æ›´æ–°ç»Ÿè®¡
            total_queries += queries_used
            total_time += attack_time
            efficiency_scores.append(efficiency_score)
            
            # åˆ›å»ºæ”»å‡»æ ·æœ¬è®°å½•
            sample = AttackSample(
                sample_id=sample_id,
                question_id=example.question_id,
                instruction=example.instruction,
                response_a=example.response_a,
                response_b=example.response_b,
                model_a=example.model_a,
                model_b=example.model_b,
                original_preference=original_preference,
                original_confidence=original_confidence,
                attack_method=best_attack_name,
                attack_action=best_action,
                modified_instruction=best_modified_instruction,  # æ·»åŠ ä¿®æ”¹åçš„æŒ‡ä»¤
                modified_response_a=best_modified_a,
                modified_response_b=best_modified_b,
                new_preference=best_new_preference,
                new_confidence=best_new_confidence,
                success=best_success,
                timestamp=time.time(),
                queries_used=queries_used,
                efficiency_score=efficiency_score,
                attack_time=attack_time,
                metadata=example.metadata
            )
            samples.append(sample)
            
            # è®°å½•åˆ°æ—¥å¿—
            if logger:
                logger.log_attack_sample(sample)
            
            # æ·»åŠ å»¶è¿Ÿ
            time.sleep(0.2)
            
        except Exception as e:
            if logger:
                logger.logger.error(f"æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
            continue
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦å˜åŒ–
    avg_confidence_change = sum(confidence_changes) / len(confidence_changes) if confidence_changes else 0.0
    
    # è®¡ç®—æ•ˆç‡ç»Ÿè®¡
    avg_queries_used = total_queries / total_samples if total_samples > 0 else 0.0
    avg_efficiency_score = sum(efficiency_scores) / len(efficiency_scores) if efficiency_scores else 0.0
    avg_attack_time = total_time / total_samples if total_samples > 0 else 0.0
    total_queries_saved = total_samples * total_action_space - total_queries
    
    # åˆ›å»ºæ”»å‡»ç»“æœ
    rl_results = AttackResults(
        attack_method="RL_Combined",
        total_samples=total_samples,
        successful_attacks=successful_attacks,
        success_rate=successful_attacks / total_samples if total_samples > 0 else 0.0,
        action_space_size=total_action_space,
        avg_confidence_change=avg_confidence_change,
        avg_queries_used=avg_queries_used,
        avg_efficiency_score=avg_efficiency_score,
        avg_attack_time=avg_attack_time,
        total_queries_saved=total_queries_saved,
        samples=samples
    )
    
    if logger:
        logger.log_rl_results(rl_results)
    
    return rl_results
"""

def main():
    """ä¸»å‡½æ•°"""
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    parser = argparse.ArgumentParser(description="Hydra-Attack ä¸»æµ‹è¯•è„šæœ¬")
    parser.add_argument("--benchmarks", nargs="+", required=True, help="è¦æµ‹è¯•çš„benchmarkåˆ—è¡¨")
    parser.add_argument("--max_samples", default=80, help="æ¯ä¸ªbenchmarkçš„æµ‹è¯•æ ·æœ¬æ•°ï¼Œä½¿ç”¨ 'full' è¡¨ç¤ºä½¿ç”¨å…¨é‡æ•°æ®")
    parser.add_argument("--attack_methods", nargs="+", default=["all"], help="æ”»å‡»æ–¹æ³•åˆ—è¡¨")
    parser.add_argument("--judge_model_path", type=str, required=True, help="Judgeæ¨¡å‹è·¯å¾„")
    parser.add_argument("--judge_type", type=str, default="qwen", help="Judgeç±»å‹")
    parser.add_argument("--results_dir", type=str, default="./results", help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--log_dir", type=str, default="./logs", help="æ—¥å¿—è¾“å‡ºç›®å½•")
    parser.add_argument("--save_detailed_results", action="store_true", help="ä¿å­˜è¯¦ç»†ç»“æœ")
    parser.add_argument("--max_retries", type=int, default=3, help="æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument("--timeout", type=int, default=300, help="è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--random_seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--baseline_max_queries", type=int, default=5, help="baselineæ–¹æ³•çš„æœ€å¤§æŸ¥è¯¢æ¬¡æ•°")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.random_seed)
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    logger = HydraLogger(log_dir=args.log_dir, results_dir=args.results_dir)
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = ExperimentConfig(
        gpu_config={
            "cuda_visible_devices": os.environ.get("CUDA_VISIBLE_DEVICES", "0"),
            "device_count": len(os.environ.get("CUDA_VISIBLE_DEVICES", "0").split(","))
        },
        data_config={
            "benchmarks": args.benchmarks,
            "max_samples": args.max_samples,
            "random_seed": args.random_seed
        },
        attack_config={
            "attack_methods": args.attack_methods,
        },
        judge_config={
            "judge_type": args.judge_type,
            "model_path": args.judge_model_path
        },
        rl_config={
            "enabled": False,
            "max_queries": args.baseline_max_queries,
            "learning_rate": 0.01,
            "exploration_rate": 0.1
        },
        output_config={
            "results_dir": args.results_dir,
            "log_dir": args.log_dir,
            "save_detailed_results": args.save_detailed_results
        },
        timestamp=time.time(),
        random_seed=args.random_seed
    )
    
    # è®°å½•é…ç½®
    logger.log_config(config)
    
    # åˆå§‹åŒ–Judge
    logger.logger.info("åˆå§‹åŒ–Judge...")
    try:
        if args.judge_type == "qwen":
            judge = create_qwen_judge(args.judge_model_path)
        elif args.judge_type == "llama":
            judge = create_llama_judge(args.judge_model_path)
        elif args.judge_type == "glm":
            judge = create_glm_judge(args.judge_model_path)   
        elif args.judge_type == "mistral":
            judge = create_mistral_judge(args.judge_model_path)
        elif args.judge_type == "gemma":
            # Check if it's 1B model (text-only)
            model_path_lower = args.judge_model_path.lower()
            if "1b" in model_path_lower or "gemma-3-1b" in model_path_lower:
                judge = create_gemma_judge_1b(args.judge_model_path)
            else:
                judge = create_gemma_judge(args.judge_model_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„Judgeç±»å‹: {args.judge_type}")
        logger.logger.info("âœ… Judgeåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.logger.error(f"âŒ Judgeåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # è·å–æ”»å‡»æ–¹æ³•
    attacks = get_attack_methods(args.attack_methods)
    logger.logger.info(f"âœ… åŠ è½½äº† {len(attacks)} ç§æ”»å‡»æ–¹æ³•")
    
    # ä¼°ç®—æ€»æ—¶é—´
    total_benchmarks = len(args.benchmarks)
    total_attacks = sum(len(attack) if isinstance(attack, dict) else 1 for attack in attacks.values())
    
    if args.max_samples == "full":
        # å¯¹äºå…¨é‡æ•°æ®ï¼Œä½¿ç”¨å¹³å‡æ ·æœ¬æ•°ä¼°ç®—
        avg_samples_per_benchmark = 1000  # ä¼°ç®—å¹³å‡å€¼
        total_samples = avg_samples_per_benchmark * total_benchmarks * total_attacks
    else:
        total_samples = args.max_samples * total_benchmarks * total_attacks
    
    estimated_time_per_sample = 1  # å‡è®¾æ¯ä¸ªæ ·æœ¬éœ€è¦1ç§’
    estimated_total_time = total_samples * estimated_time_per_sample
    
    hours = int(estimated_total_time // 3600)
    minutes = int((estimated_total_time % 3600) // 60)
    
    logger.logger.info(f"ğŸ“Š é¢„ä¼°ç»Ÿè®¡:")
    logger.logger.info(f"   - Benchmarkæ•°é‡: {total_benchmarks}")
    logger.logger.info(f"   - æ”»å‡»æ–¹æ³•æ•°é‡: {total_attacks}")
    logger.logger.info(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
    logger.logger.info(f"   - é¢„ä¼°æ€»æ—¶é—´: {hours:02d}:{minutes:02d}:00")
    logger.logger.info("=" * 60)
    
    # æµ‹è¯•æ¯ä¸ªbenchmark
    # æ·»åŠ æ€»ä½“è¿›åº¦æ¡
    benchmark_progress = tqdm(
        args.benchmarks, 
        desc="å¤„ç†Benchmark",
        unit="benchmark",
        ncols=100,
        position=0
    )
    
    for benchmark in benchmark_progress:
        benchmark_progress.set_description(f"å¤„ç† {benchmark}")
        logger.log_benchmark_start(benchmark, args.max_samples)
        
        # åŠ è½½æ•°æ®
        examples = load_benchmark_data(benchmark, args.max_samples, args.random_seed)
        if not examples:
            logger.logger.error(f"âŒ æ— æ³•åŠ è½½ {benchmark} æ•°æ®")
            continue
        
        logger.logger.info(f"âœ… åŠ è½½äº† {len(examples)} ä¸ªæ ·æœ¬")
        
        # å°è¯•ä»æ•°æ®åˆ†å‰²ä¿¡æ¯æ–‡ä»¶ä¸­è¯»å–train_ratioå’Œtest_ratio
        train_ratio = None
        test_ratio = None
        split_info_file = f"data/split/{benchmark}_split_info.json"
        if os.path.exists(split_info_file):
            try:
                with open(split_info_file, 'r') as f:
                    split_info = json.load(f)
                    train_ratio = split_info.get("train_ratio")
                    test_ratio = split_info.get("test_ratio")
            except Exception as e:
                logger.logger.warning(f"âš ï¸  æ— æ³•è¯»å–æ•°æ®åˆ†å‰²ä¿¡æ¯: {e}")
        
        # æ„å»ºå®éªŒé…ç½®ä¿¡æ¯
        experiment_config = {
            "cuda_visible_devices": os.environ.get("CUDA_VISIBLE_DEVICES", "unknown"),
            "max_samples": args.max_samples,
            "train_ratio": train_ratio,
            "test_ratio": test_ratio,
            "judge_model_path": args.judge_model_path,
            "judge_type": args.judge_type,
            "random_seed": args.random_seed,
            "baseline_max_queries": args.baseline_max_queries,
            "attack_methods": args.attack_methods,
            "benchmark": benchmark,
            "total_samples": len(examples)
        }
        
        # æµ‹è¯•æ”»å‡»æ–¹æ³•
        baseline_results = test_attacks(examples, judge, attacks, args.max_samples, logger, 
                                       args.baseline_max_queries, args.results_dir, benchmark,
                                       experiment_config)
        
        # è®¡ç®—benchmarkç»“æœ
        total_successful_attacks = sum(ar.successful_attacks for ar in baseline_results.values()) if baseline_results else 0
        total_attack_attempts = sum(ar.total_samples for ar in baseline_results.values()) if baseline_results else 0
        overall_success_rate = total_successful_attacks / total_attack_attempts if total_attack_attempts > 0 else 0.0
        overall_success_rate = round(overall_success_rate, 4)  # ä¿ç•™å››ä½å°æ•°ç²¾åº¦
        
        # æ‰¾åˆ°æœ€ä½³å’Œæœ€å·®æ”»å‡»æ–¹æ³•
        best_attack_method = ""
        worst_attack_method = ""
        if baseline_results:
            sorted_results = sorted(baseline_results.items(), key=lambda x: x[1].success_rate, reverse=True)
            best_attack_method = sorted_results[0][0] if sorted_results else ""
            worst_attack_method = sorted_results[-1][0] if sorted_results else ""
        
        # åˆ›å»ºbenchmarkç»“æœ
        benchmark_results = BenchmarkResults(
            benchmark_name=benchmark,
            total_samples=len(examples),
            baseline_results=baseline_results,
            rl_results=None,  # æ˜ç¡®è®¾ç½®ä¸ºNone
            overall_success_rate=overall_success_rate,
            best_attack_method=best_attack_method,
            worst_attack_method=worst_attack_method
        )
        
        # è®°å½•benchmarkç»“æœ
        logger.log_benchmark_results(benchmark_results)
        
        # æ³¨æ„ï¼šæ¯ä¸ªæ”»å‡»æ–¹æ³•çš„ç»“æœæ–‡ä»¶å·²ç»åœ¨æµ‹è¯•å®Œæˆåç«‹å³ä¿å­˜ï¼Œæ— éœ€é‡å¤ä¿å­˜
        
        # ä¿å­˜å®éªŒé…ç½®
        config_file = os.path.join(args.results_dir, "experiment_config.json")
        config_data = {
            "benchmark": benchmark,
            "timestamp": time.strftime("%Y%m%d_%H%M%S"),
            "max_samples": args.max_samples,
            "max_queries": args.baseline_max_queries,
            "judge_model_path": args.judge_model_path,
            "judge_type": args.judge_type,
            "random_seed": args.random_seed,
            "attack_methods": list(baseline_results.keys()),
            "total_baseline_methods": len(baseline_results)
        }
        
        with open(config_file, 'w') as f:
            json.dump(config_data, f, indent=2)
        
        logger.logger.info(f"ğŸ’¾ å®éªŒé…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    
    # è®°å½•å®éªŒæ€»ç»“
    logger.log_experiment_summary()
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - start_time
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = int(total_time % 60)
    
    logger.logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    logger.logger.info(f"â±ï¸  æ€»è€—æ—¶: {hours:02d}:{minutes:02d}:{seconds:02d}")
    logger.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {logger.get_log_file_path()}")
    logger.logger.info(f"ğŸ“Š ç»“æœæ–‡ä»¶: {logger.get_results_file_path()}")
    logger.logger.info(f"ğŸ“ æ ·æœ¬è¯¦æƒ…: {logger.get_samples_file_path()}")


if __name__ == "__main__":
    main()